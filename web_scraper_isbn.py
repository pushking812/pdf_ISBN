import asyncio
import aiohttp
import re
import time
import random
from typing import Optional, Dict, Any, Tuple, List
from enum import Enum
import requests
from requests.exceptions import RequestException
import isbnlib

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, InvalidSessionIdException, WebDriverException
from bs4 import BeautifulSoup


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
class ScraperConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞."""
    def __init__(self,
                 headless: bool = False,
                 base_url: str = "https://www.chitai-gorod.ru",
                 skip_main_page: bool = False,
                 use_fast_selectors: bool = False,
                 # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏
                 delay_after_main: Tuple[float, float] = (1.5, 2.5),
                 delay_after_search: Tuple[float, float] = (2.0, 3.0),
                 delay_after_click: Tuple[float, float] = (1.5, 2.5),
                 delay_between_actions: Tuple[float, float] = (0.3, 0.7),
                 wait_city_modal: int = 3,
                 wait_product_link: int = 6,
                 # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
                 poll_interval: float = 0.5,
                 # –§—Ä–∞–∑—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–æ–≤–∞—Ä–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
                 no_product_phrases: List[str] = None,
                 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∫–ª–∞–¥–æ–∫
                 max_tabs: int = 5,
                 # –§—Ä–∞–∑—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É (Too many requests)
                 rate_limit_phrases: List[str] = None,
                 # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ (—Å–µ–∫)
                 rate_limit_initial_delay: float = 10.0,
                 # –ù–∞—á–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–Ω–æ–∂–∏—Ç–µ–ª—è
                 rate_limit_coef_start: float = 1.0,
                 # –®–∞–≥ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞
                 rate_limit_coef_step: float = 0.2,
                 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–Ω–æ–∂–∏—Ç–µ–ª—è
                 rate_limit_coef_max: float = 3.0,
                 # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É (–µ—Å–ª–∏ False, —Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)
                 handle_rate_limit: bool = True,
                # –û—Å—Ç–∞–≤–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä –æ—Ç–∫—Ä—ã—Ç—ã–º –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
                keep_browser_open: bool = False,
                # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                verbose: bool = False,
                # –ú–∞–∫—Å. –∫–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö ISBN –Ω–∞ —ç—Ç–∞–ø–µ API/–†–ì–ë (—Å–Ω–∏–∂–∞–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –†–ì–ë/API)
                api_max_concurrent: int = 5):
        self.headless = headless
        self.base_url = base_url
        self.skip_main_page = skip_main_page
        self.use_fast_selectors = use_fast_selectors
        self.delay_after_main = delay_after_main
        self.delay_after_search = delay_after_search
        self.delay_after_click = delay_after_click
        self.delay_between_actions = delay_between_actions
        self.wait_city_modal = wait_city_modal
        self.wait_product_link = wait_product_link
        self.poll_interval = poll_interval
        self.no_product_phrases = no_product_phrases or [
            "–ü–æ—Ö–æ–∂–µ, —É –Ω–∞—Å —Ç–∞–∫–æ–≥–æ –Ω–µ—Ç",
            "–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å"
        ]
        self.max_tabs = max_tabs
        self.rate_limit_phrases = rate_limit_phrases or [
            "DDoS-Guard",
            "DDOS",
            "Checking your browser",
            "–î–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω"
        ]
        self.rate_limit_initial_delay = rate_limit_initial_delay
        self.rate_limit_coef_start = rate_limit_coef_start
        self.rate_limit_coef_step = rate_limit_coef_step
        self.rate_limit_coef_max = rate_limit_coef_max
        self.handle_rate_limit = handle_rate_limit
        self.keep_browser_open = keep_browser_open
        self.verbose = verbose
        self.api_max_concurrent = api_max_concurrent


# ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø ISBN ====================
def normalize_isbn(isbn: str) -> Optional[str]:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç ISBN (10 –∏–ª–∏ 13 –∑–Ω–∞–∫–æ–≤, —Å –¥–µ—Ñ–∏—Å–∞–º–∏ –∏–ª–∏ –±–µ–∑),
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π 13-–∑–Ω–∞—á–Ω—ã–π –∫–æ–¥ (–±–µ–∑ –¥–µ—Ñ–∏—Å–æ–≤) –∏–ª–∏ None, –µ—Å–ª–∏ –∫–æ–¥ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω.
    """
    clean = isbnlib.canonical(isbn)
    if not clean:
        return None
    if isbnlib.is_isbn13(clean):
        return clean
    if isbnlib.is_isbn10(clean):
        return isbnlib.to_isbn13(clean)
    return None


# ==================== –ê–°–ò–ù–•–†–û–ù–ù–´–ï API –§–£–ù–ö–¶–ò–ò ====================
async def get_from_google_books_async(session: aiohttp.ClientSession, isbn: str) -> Optional[Dict[str, Any]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–Ω–∏–≥–∏ –≤ Google Books API –ø–æ ISBN."""
    url = "https://www.googleapis.com/books/v1/volumes"
    params = {"q": f"isbn:{isbn}", "maxResults": 1}
    try:
        async with session.get(url, params=params, timeout=10) as response:
            if response.status != 200:
                return None
            data = await response.json()
            if data.get("totalItems", 0) == 0:
                return None
            volume = data["items"][0]["volumeInfo"]
            title = volume.get("title", "–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è")
            authors = volume.get("authors", ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"])

            pages = volume.get("pageCount")
            if pages is not None:
                pages = str(pages)
            else:
                pages = None

            year = None
            published_date = volume.get("publishedDate")
            if published_date:
                match = re.search(r'\d{4}', published_date)
                if match:
                    year = match.group()

            return {
                "title": title,
                "authors": authors,
                "source": "Google Books",
                "pages": pages,
                "year": year
            }
    except Exception:
        return None


async def get_from_open_library_async(session: aiohttp.ClientSession, isbn: str) -> Optional[Dict[str, Any]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–Ω–∏–≥–∏ –≤ Open Library API –ø–æ ISBN."""
    url = "https://openlibrary.org/api/books"
    params = {
        "bibkeys": f"ISBN:{isbn}",
        "format": "json",
        "jscmd": "data"
    }
    headers = {"User-Agent": "BookSearcher/1.0 (contact@example.com)"}
    try:
        async with session.get(url, params=params, headers=headers, timeout=10) as response:
            if response.status != 200:
                return None
            data = await response.json()
            key = f"ISBN:{isbn}"
            if key not in data:
                return None
            book_data = data[key]
            title = book_data.get("title", "–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è")
            authors = [a["name"] for a in book_data.get("authors", [])]
            if not authors:
                authors = ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]

            year = None
            if "publish_date" in book_data:
                m = re.search(r'\d{4}', book_data["publish_date"])
                if m:
                    year = m.group()

            pages = book_data.get("number_of_pages")
            if pages is not None:
                pages = str(pages)

            return {
                "title": title,
                "authors": authors,
                "source": "Open Library",
                "pages": pages,
                "year": year
            }
    except Exception:
        return None


async def get_from_rsl_async(session: aiohttp.ClientSession, isbn: str) -> Optional[Dict[str, Any]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–Ω–∏–≥–∏ –≤ –†–æ—Å—Å–∏–π—Å–∫–æ–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ (–†–ì–ë) –ø–æ ISBN.
    """
    url = "https://search.rsl.ru/ru/search"
    params = {"q": isbn}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        async with session.get(url, params=params, headers=headers, timeout=15) as response:
            if response.status != 200:
                return None
            html = await response.text()
            soup = BeautifulSoup(html, 'lxml')

            containers = soup.find_all('div', class_='search-container')
            if not containers:
                return None

            first = containers[0]

            author_tag = first.find('b', class_='js-item-authorinfo')
            authors = []
            if author_tag:
                authors = [author_tag.text.strip().rstrip('.')]
            else:
                authors = ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]

            desc_span = first.find('span', class_='js-item-maininfo')
            if not desc_span:
                return None
            description = desc_span.text.strip()

            title = description.split(' / ')[0].strip()
            if not title:
                title = "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ"

            year = None
            year_match = re.search(r',\s*(\d{4})\.', description)
            if year_match:
                year = year_match.group(1)

            pages = None
            pages_match = re.search(r'\.\s*-\s*(\d+)\s+—Å\.', description)
            if pages_match:
                pages = pages_match.group(1)

            return {
                "title": title,
                "authors": authors,
                "source": "–†–ì–ë",
                "pages": pages or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
                "year": year or "–Ω–µ —É–∫–∞–∑–∞–Ω"
            }
    except Exception:
        return None


# ==================== –°–ò–ù–•–†–û–ù–ù–´–ï –û–ë–Å–†–¢–ö–ò –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (–ú–û–ñ–ù–û –û–°–¢–ê–í–ò–¢–¨) ====================
# (–ù–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏, –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞)

def get_from_google_books(isbn: str) -> Optional[Dict[str, Any]]:
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    import requests
    url = "https://www.googleapis.com/books/v1/volumes"
    params = {"q": f"isbn:{isbn}", "maxResults": 1}
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("totalItems", 0) == 0:
            return None
        volume = data["items"][0]["volumeInfo"]
        title = volume.get("title", "–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è")
        authors = volume.get("authors", ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"])
        pages = volume.get("pageCount")
        if pages is not None:
            pages = str(pages)
        else:
            pages = None
        year = None
        published_date = volume.get("publishedDate")
        if published_date:
            match = re.search(r'\d{4}', published_date)
            if match:
                year = match.group()
        return {
            "title": title,
            "authors": authors,
            "source": "Google Books",
            "pages": pages,
            "year": year
        }
    except Exception:
        return None


def get_from_open_library(isbn: str) -> Optional[Dict[str, Any]]:
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    import requests
    url = "https://openlibrary.org/api/books"
    params = {"bibkeys": f"ISBN:{isbn}", "format": "json", "jscmd": "data"}
    headers = {"User-Agent": "BookSearcher/1.0 (contact@example.com)"}
    try:
        response = requests.get(url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        key = f"ISBN:{isbn}"
        if key not in data:
            return None
        book_data = data[key]
        title = book_data.get("title", "–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è")
        authors = [a["name"] for a in book_data.get("authors", [])]
        if not authors:
            authors = ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]
        year = None
        if "publish_date" in book_data:
            m = re.search(r'\d{4}', book_data["publish_date"])
            if m:
                year = m.group()
        pages = book_data.get("number_of_pages")
        if pages is not None:
            pages = str(pages)
        return {
            "title": title,
            "authors": authors,
            "source": "Open Library",
            "pages": pages,
            "year": year
        }
    except Exception:
        return None


def get_from_rsl(isbn: str) -> Optional[Dict[str, Any]]:
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    import requests
    from bs4 import BeautifulSoup
    url = "https://search.rsl.ru/ru/search"
    params = {"q": isbn}
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        response = requests.get(url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        containers = soup.find_all('div', class_='search-container')
        if not containers:
            return None
        first = containers[0]
        author_tag = first.find('b', class_='js-item-authorinfo')
        authors = [author_tag.text.strip().rstrip('.')] if author_tag else ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]
        desc_span = first.find('span', class_='js-item-maininfo')
        if not desc_span:
            return None
        description = desc_span.text.strip()
        title = description.split(' / ')[0].strip() or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ"
        year = None
        year_match = re.search(r',\s*(\d{4})\.', description)
        if year_match:
            year = year_match.group(1)
        pages = None
        pages_match = re.search(r'\.\s*-\s*(\d+)\s+—Å\.', description)
        if pages_match:
            pages = pages_match.group(1)
        return {
            "title": title,
            "authors": authors,
            "source": "–†–ì–ë",
            "pages": pages or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
            "year": year or "–Ω–µ —É–∫–∞–∑–∞–Ω"
        }
    except Exception:
        return None


# ==================== –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –°–ö–†–ê–ü–ï–†–ê ====================
class TabState(Enum):
    INIT = 0
    SEARCHING = 1
    BOOK_PAGE = 2
    DONE = 3
    ERROR = 4
    RATE_LIMITED = 5


class TabInfo:
    def __init__(self, isbn: str, handle: str, index: int, config: ScraperConfig):
        self.isbn = isbn
        self.handle = handle
        self.index = index
        self.state = TabState.INIT
        self.result = None
        self.error = None
        self.book_url = None
        self.search_start_time = None
        self.timeout = config.wait_product_link


class RussianBookScraperUC:
    """–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Undetected ChromeDriver."""
    def __init__(self, config: ScraperConfig):
        self.config = config
        self.driver = None
        self._init_selectors()

    def _init_selectors(self):
        if self.config.use_fast_selectors:
            self.product_link_selectors = ['a[href^="/product/"]']
            self.title_selectors = ['h1.product-detail-page__title', 'h1.product-title']
            self.author_selectors = ['.product-authors a']
            self.pages_selectors = ['span[itemprop="numberOfPages"] span']
            self.year_selectors = ['span[itemprop="datePublished"] span']
        else:
            self.product_link_selectors = [
                'a[href^="/product/"]',
                'a.product-card__title',
                'a.product-title',
                '.catalog-item a'
            ]
            self.title_selectors = [
                'h1.product-detail-page__title',
                'h1.product-title',
                'h1[itemprop="name"]',
                '.product__title h1',
                'h1'
            ]
            self.author_selectors = [
                '.product-authors a',
                '.product-author a',
                'a[itemprop="author"]',
                '.product-info__author',
                '.authors-list a'
            ]
            self.pages_selectors = [
                'span[itemprop="numberOfPages"] span',
                '.product-properties-item span[itemprop="numberOfPages"]'
            ]
            self.year_selectors = [
                'span[itemprop="datePublished"] span',
                '.product-properties-item span[itemprop="datePublished"]'
            ]
        self.properties_item_class = 'product-properties-item'
        self.properties_title_class = 'product-properties-item__title'
        self.properties_content_class = 'product-properties-item__content'

    def __enter__(self):
        self.driver = uc.Chrome(headless=self.config.headless)
        self.driver.set_window_size(1920, 1080)
        self.driver.execute_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver and not self.config.keep_browser_open:
            self.driver.quit()

    def _random_delay(self, delay_range: Tuple[float, float], msg: str = ""):
        delay = random.uniform(*delay_range)
        if msg and self.config.verbose:
            print(f"‚è±Ô∏è {msg}: {delay:.2f}—Å")
        time.sleep(delay)

    def _handle_city_modal(self):
        try:
            city_button = WebDriverWait(self.driver, self.config.wait_city_modal).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), '–î–∞, —è –∑–¥–µ—Å—å')]"))
            )
            city_button.click()
            if self.config.verbose:
                print("üèôÔ∏è –ì–æ—Ä–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω")
            self._random_delay(self.config.delay_between_actions, "–ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞")
            return True
        except:
            return False

    def _parse_book_page(self) -> Dict[str, Any]:
        soup = BeautifulSoup(self.driver.page_source, 'lxml')
        title = None
        for sel in self.title_selectors:
            elem = soup.select_one(sel)
            if elem:
                title = elem.text.strip()
                break

        authors = []
        for sel in self.author_selectors:
            elems = soup.select(sel)
            if elems:
                authors = [a.text.strip() for a in elems if a.text.strip()]
                break

        pages = None
        for sel in self.pages_selectors:
            elem = soup.select_one(sel)
            if elem:
                pages = elem.text.strip()
                break

        year = None
        for sel in self.year_selectors:
            elem = soup.select_one(sel)
            if elem:
                year = elem.text.strip()
                break

        if not pages or not year:
            props = soup.find_all('li', class_=self.properties_item_class)
            for li in props:
                title_elem = li.find('span', class_=self.properties_title_class)
                if not title_elem:
                    continue
                text = title_elem.text.strip()
                content_elem = li.find('span', class_=self.properties_content_class)
                if not content_elem:
                    continue
                if '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü' in text and not pages:
                    pages = content_elem.text.strip()
                elif '–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è' in text and not year:
                    year = content_elem.text.strip()

        title = title or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ"
        authors = authors or ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]
        pages = pages or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"
        year = year or "–Ω–µ —É–∫–∞–∑–∞–Ω"

        return {
            'title': title,
            'authors': authors,
            'pages': pages,
            'year': year,
            'url': self.driver.current_url,
            'source': '–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥'
        }

    def search_by_isbn(self, isbn: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É ISBN (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –¥–ª—è –æ–¥–Ω–æ–π –≤–∫–ª–∞–¥–∫–∏)."""
        clean_isbn = isbn.replace("-", "").strip()
        search_url = f"{self.config.base_url}/search?phrase={clean_isbn}"

        try:
            if not self.config.skip_main_page:
                self.driver.get(self.config.base_url)
                self._random_delay(self.config.delay_after_main, "–ø–æ—Å–ª–µ –≥–ª–∞–≤–Ω–æ–π")
                self._handle_city_modal()
            else:
                if self.config.verbose:
                    print("‚è© –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (skip_main_page=True)")

            self.driver.get(search_url)
            self._random_delay(self.config.delay_after_search, "–ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞")
            self._handle_city_modal()

            product_link = None
            for selector in self.product_link_selectors:
                try:
                    product_link = WebDriverWait(self.driver, self.config.wait_product_link).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    break
                except TimeoutException:
                    continue

            if not product_link:
                if self.config.verbose:
                    print("‚ùå –°—Å—ã–ª–∫–∞ –Ω–∞ –∫–Ω–∏–≥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
                return None

            book_url = product_link.get_attribute('href')
            self.driver.get(book_url)
            self._random_delay(self.config.delay_after_click, "–ø–æ—Å–ª–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ –∫–Ω–∏–≥—É")
            return self._parse_book_page()

        except Exception as e:
            if self.config.verbose:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return None


# ==================== –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–û–ò–°–ö (–ß–ê–ù–ö–û–í–´–ô –ê–õ–ì–û–†–ò–¢–ú –ò–ó scrapper14) ====================
def async_parallel_search(isbn_list: List[str], config: Optional[ScraperConfig] = None) -> List[Optional[Dict[str, Any]]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö ISBN –≤ –æ–¥–Ω–æ–º –±—Ä–∞—É–∑–µ—Ä–µ —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏.
    –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ scrapper14.
    """
    if config is None:
        config = ScraperConfig()

    # –°–æ–∑–¥–∞—ë–º –¥—Ä–∞–π–≤–µ—Ä (–≤–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞, —á—Ç–æ–±—ã —É–ø—Ä–∞–≤–ª—è—Ç—å –≤—Ä—É—á–Ω—É—é)
    driver = uc.Chrome(headless=config.headless)
    driver.set_window_size(1920, 1080)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not config.skip_main_page:
        driver.get(config.base_url)
        time.sleep(random.uniform(*config.delay_after_main))
        try:
            WebDriverWait(driver, config.wait_city_modal).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), '–î–∞, —è –∑–¥–µ—Å—å')]"))
            ).click()
            if config.verbose:
                print("üèôÔ∏è –ì–æ—Ä–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω (–≥–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ)")
            time.sleep(random.uniform(*config.delay_between_actions))
        except:
            pass
    else:
        if config.verbose:
            print("‚è© –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ ISBN –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ max_tabs
    chunks = [isbn_list[i:i + config.max_tabs] for i in range(0, len(isbn_list), config.max_tabs)]
    all_results = []
    rate_limit_attempts = 0

    # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç scraper –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
    scraper_template = RussianBookScraperUC(config)

    for chunk_idx, chunk in enumerate(chunks):
        if config.verbose:
            print(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {chunk_idx + 1}/{len(chunks)} (ISBN: {chunk}) ===")

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
        handles = []
        try:
            main_handle = driver.current_window_handle
            handles.append(main_handle)
            for i in range(1, len(chunk)):
                driver.switch_to.new_window('tab')
                time.sleep(0.5)
                new_handle = driver.current_window_handle
                if new_handle not in handles:
                    handles.append(new_handle)
                else:
                    all_handles = driver.window_handles
                    found = False
                    for h in all_handles:
                        if h not in handles:
                            handles.append(h)
                            found = True
                            break
                    if not found:
                        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–π handle –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ {i}")
        except Exception as e:
            if config.verbose:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∫–ª–∞–¥–æ–∫: {e}")
            all_results.extend([None] * len(chunk))
            time.sleep(1)
            continue

        tabs = [TabInfo(chunk[i], handles[i], i, config) for i in range(len(chunk))]

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞–∂–¥—É—é –≤–∫–ª–∞–¥–∫—É
        for tab in tabs:
            try:
                driver.switch_to.window(tab.handle)
                clean_isbn = tab.isbn.replace("-", "").strip()
                search_url = f"{config.base_url}/search?phrase={clean_isbn}"
                driver.get(search_url)
                tab.state = TabState.SEARCHING
                tab.search_start_time = time.time()
                time.sleep(0.2)
            except Exception as e:
                if config.verbose:
                    print(f"‚ùå [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–∏—Å–∫: {e}")
                tab.state = TabState.ERROR

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞
        all_done = False
        while not all_done:
            all_done = True
            for tab in tabs:
                if tab.state in (TabState.DONE, TabState.ERROR, TabState.RATE_LIMITED):
                    continue
                all_done = False

                try:
                    driver.switch_to.window(tab.handle)
                except Exception as e:
                    if config.verbose:
                        print(f"‚ùå [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è: {e}")
                    tab.state = TabState.ERROR
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if config.handle_rate_limit:
                    try:
                        page_source = driver.page_source
                        found_rate_limit = any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases)
                        if found_rate_limit:
                            if config.verbose:
                                print(f"‚ö†Ô∏è [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                            rate_limit_attempts += 1
                            coef = config.rate_limit_coef_start + (rate_limit_attempts - 1) * config.rate_limit_coef_step
                            coef = min(coef, config.rate_limit_coef_max)
                            wait_time = config.rate_limit_initial_delay * coef
                            if config.verbose:
                                print(f"‚è∏Ô∏è –ü–∞—É–∑–∞ {wait_time:.1f}—Å (–∫–æ—ç—Ñ. {coef:.2f})")
                            time.sleep(wait_time)
                            driver.refresh()
                            while True:
                                time.sleep(config.poll_interval)
                                page_source = driver.page_source
                                if any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases):
                                    time.sleep(wait_time)
                                    driver.refresh()
                                else:
                                    if config.verbose:
                                        print(f"‚Üª –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–Ω—è—Ç–∞, —Å–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞")
                                    rate_limit_attempts = 0
                                    break
                            for t in tabs:
                                if t.handle != tab.handle:
                                    try:
                                        driver.switch_to.window(t.handle)
                                        driver.refresh()
                                        time.sleep(0.5)
                                    except:
                                        pass
                            driver.switch_to.window(tab.handle)
                            break
                        else:
                            if rate_limit_attempts > 0:
                                rate_limit_attempts = 0
                    except Exception:
                        pass

                if tab.state == TabState.SEARCHING:
                    try:
                        page_source = driver.page_source
                        if any(phrase in page_source for phrase in config.no_product_phrases):
                            if config.verbose:
                                print(f"‚ùå [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
                            tab.state = TabState.ERROR
                            continue
                    except:
                        pass

                    elapsed = time.time() - tab.search_start_time
                    found_link = None
                    for selector in scraper_template.product_link_selectors:
                        try:
                            element = driver.find_element(By.CSS_SELECTOR, selector)
                            found_link = element
                            break
                        except NoSuchElementException:
                            continue
                    if found_link:
                        tab.book_url = found_link.get_attribute('href')
                        driver.get(tab.book_url)
                        time.sleep(random.uniform(*config.delay_after_click))
                        tab.state = TabState.BOOK_PAGE
                        if config.verbose:
                            print(f"üìñ [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏")
                    else:
                        if elapsed > tab.timeout:
                            if config.verbose:
                                print(f"‚ùå [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç")
                            tab.state = TabState.ERROR

                elif tab.state == TabState.BOOK_PAGE:
                    try:
                        scraper_template.driver = driver
                        result = scraper_template._parse_book_page()
                        tab.result = result
                        tab.state = TabState.DONE
                        rate_limit_attempts = 0
                        if config.verbose:
                            print(f"\n‚úÖ [–í–∫–ª–∞–¥–∫–∞ {tab.index}] ISBN {tab.isbn} –≥–æ—Ç–æ–≤")
                    except Exception as e:
                        if config.verbose:
                            print(f"‚ùå [–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                        tab.state = TabState.ERROR

            if not all_done:
                time.sleep(config.poll_interval)

        # –ó–∞–∫—Ä—ã—Ç–∏–µ –≤–∫–ª–∞–¥–æ–∫ —á–∞–Ω–∫–∞ (–∫—Ä–æ–º–µ –≥–ª–∞–≤–Ω–æ–π)
        main_handle = handles[0]
        for handle in handles[1:]:
            try:
                driver.switch_to.window(handle)
                driver.close()
            except:
                pass
        driver.switch_to.window(main_handle)
        all_results.extend([tab.result for tab in tabs])
        time.sleep(1)

    if not config.keep_browser_open:
        driver.quit()
    else:
        input("\nüîç –ë—Ä–∞—É–∑–µ—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–º. –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è...")
        driver.quit()

    return all_results


# ==================== –ê–°–ò–ù–•–†–û–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –≠–¢–ê–ü–ê API/–†–ì–ë ====================
async def process_isbn_async(
    session: aiohttp.ClientSession,
    raw_isbn: str,
    idx: int,
    config: ScraperConfig,
    semaphore: asyncio.Semaphore,
) -> Tuple[int, Optional[Dict[str, Any]]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω ISBN: –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫–æ –≤—Å–µ–º —Ç—Ä—ë–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ None.
    –°–µ–º–∞—Ñ–æ—Ä –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —á–∏—Å–ª–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö ISBN, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –†–ì–ë/API.
    """
    norm_isbn = normalize_isbn(raw_isbn)
    if not norm_isbn:
        return idx, None

    async with semaphore:
        tasks = [
            get_from_google_books_async(session, norm_isbn),
            get_from_open_library_async(session, norm_isbn),
            get_from_rsl_async(session, raw_isbn)   # –†–ì–ë –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π ISBN
        ]
        results = await asyncio.gather(*tasks)

    for res in results:
        if res:
            return idx, res
    return idx, None


async def run_api_stage(isbn_list: List[str], config: ScraperConfig) -> Tuple[List[Optional[Dict[str, Any]]], List[str], List[int]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —ç—Ç–∞–ø API/–†–ì–ë –¥–ª—è –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ ISBN.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - results: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä —Ä–∞–≤–µ–Ω len(isbn_list), None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)
      - remaining_isbns: —Å–ø–∏—Å–æ–∫ ISBN, –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ
      - remaining_indices: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã
    """
    results = [None] * len(isbn_list)
    remaining_isbns = []
    remaining_indices = []

    total = len(isbn_list)
    print("\nüîç –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API –∏ –†–ì–ë (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ):")
    header = f"{' ‚Ññ':>4} | {'ISBN':<20} | {'Google Books':<12} | {'Open Library':<12} | {'–†–ì–ë':<8} | –°—Ç–∞—Ç—É—Å"
    print(header)
    print("-" * len(header))

    semaphore = asyncio.Semaphore(config.api_max_concurrent)
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ —Ö–æ—Å—Ç, —á—Ç–æ–±—ã –†–ì–ë/API –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏ –ø—Ä–∏ –≤—Å–ø–ª–µ—Å–∫–∞—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    connector = aiohttp.TCPConnector(limit_per_host=config.api_max_concurrent * 2)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [process_isbn_async(session, isbn_list[i], i, config, semaphore) for i in range(total)]
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ (—Å–µ–º–∞—Ñ–æ—Ä), —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –†–ì–ë/API
        for future in asyncio.as_completed(tasks):
            idx, res = await future
            raw_isbn = isbn_list[idx]
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ç–æ—á–Ω–æ, –º–æ–∂–Ω–æ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É)
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤—ã–≤–µ–¥–µ–º –ø–æ —Ñ–∞–∫—Ç—É –Ω–∞–ª–∏—á–∏—è res
            google_status = "‚úÖ" if res and res['source'] == 'Google Books' else "‚ùå"
            open_status = "‚úÖ" if res and res['source'] == 'Open Library' else "‚ùå"
            rsl_status = "‚úÖ" if res and res['source'] == '–†–ì–ë' else "‚ùå"
            if res:
                status_msg = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ ({res['source']})"
                results[idx] = res
            else:
                status_msg = "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ"
                remaining_isbns.append(raw_isbn)
                remaining_indices.append(idx)
            # –í—ã–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã (–Ω—É–º–µ—Ä—É–µ–º –æ—Ç 1 –¥–æ total)
            print(f"{idx+1:4} | {raw_isbn:<20} | {google_status:^12} | {open_status:^12} | {rsl_status:^8} | {status_msg}")

    return results, remaining_isbns, remaining_indices


# ==================== –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê ====================
def search_book_by_isbn(isbn: str, config: Optional[ScraperConfig] = None) -> Optional[Dict[str, Any]]:
    """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –æ–¥–Ω–æ–π –∫–Ω–∏–≥–∏ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞)."""
    norm_isbn = normalize_isbn(isbn)
    if not norm_isbn:
        print(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ISBN: {isbn}")
        return None

    print(f"üîç {isbn}:")
    res = get_from_google_books(norm_isbn)
    if res:
        print(f"   ‚Üí Google Books ‚úÖ {res['title']}")
        return res
    else:
        print("   ‚Üí Google Books ‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    res = get_from_open_library(norm_isbn)
    if res:
        print(f"   ‚Üí Open Library ‚úÖ {res['title']}")
        return res
    else:
        print("   ‚Üí Open Library ‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    res = get_from_rsl(isbn)
    if res:
        print(f"   ‚Üí –†–ì–ë ‚úÖ {res['title']}")
        return res
    else:
        print("   ‚Üí –†–ì–ë ‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    print(f"   ‚Üí –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è ISBN {isbn}")
    if config is None:
        config = ScraperConfig()
    with RussianBookScraperUC(config) as scraper:
        return scraper.search_by_isbn(isbn)


def search_multiple_books(isbn_list: List[str], config: Optional[ScraperConfig] = None) -> List[Optional[Dict[str, Any]]]:
    """
    –ü–æ–∏—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö ISBN: —Å–Ω–∞—á–∞–ª–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —ç—Ç–∞–ø API –∏ –†–ì–ë (—á–µ—Ä–µ–∑ asyncio),
    –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è ‚Äî –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥.
    """
    if config is None:
        config = ScraperConfig()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —ç—Ç–∞–ø API/–†–ì–ë
    results, remaining_isbns, remaining_indices = asyncio.run(run_api_stage(isbn_list, config))

    if remaining_indices:
        print(f"\nüîç –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è {len(remaining_indices)} ISBN, –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ API/–†–ì–ë")
        scraped_results = async_parallel_search(remaining_isbns, config)
        for idx, res in zip(remaining_indices, scraped_results):
            results[idx] = res

    return results


# ==================== –¢–ê–ë–õ–ò–ß–ù–´–ô –í–´–í–û–î ====================
def print_results_table(isbn_list: List[str], results: List[Optional[Dict[str, Any]]]):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    print("\n" + "="*130)
    header = f"{'ISBN':<20} {'–ù–∞–∑–≤–∞–Ω–∏–µ':<40} {'–ê–≤—Ç–æ—Ä(—ã)':<30} {'–°—Ç—Ä.':<6} {'–ì–æ–¥':<5} {'–ò—Å—Ç–æ—á–Ω–∏–∫':<15}"
    print(header)
    print("="*130)
    for i, res in enumerate(results):
        if res:
            authors_str = ", ".join(res.get('authors', []))
            title = (res['title'][:37] + "...") if len(res['title']) > 40 else res['title']
            authors = (authors_str[:27] + "...") if len(authors_str) > 30 else authors_str

            pages = res.get('pages')
            if pages is None:
                pages = '‚Äî'
            else:
                pages = str(pages)

            year = res.get('year')
            if year is None:
                year = '‚Äî'
            else:
                year = str(year)

            source = res.get('source')
            if source is None:
                source = '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            else:
                source = str(source)

            print(f"{isbn_list[i]:<20} {title:<40} {authors:<30} {pages:<6} {year:<5} {source:<15}")
        else:
            print(f"{isbn_list[i]:<20} {'‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω–∞':<40} {'':<30} {'':<6} {'':<5} {'':<15}")
    print("="*130)


# ==================== –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ====================
if __name__ == "__main__":
    config = ScraperConfig(
        headless=False,
        skip_main_page=True,
        use_fast_selectors=True,
        wait_product_link=6,
        delay_after_main=(0.5, 1.0),
        delay_after_search=(0.8, 1.5),
        delay_after_click=(0.5, 1.0),
        poll_interval=0.5,
        no_product_phrases=["–ü–æ—Ö–æ–∂–µ, —É –Ω–∞—Å —Ç–∞–∫–æ–≥–æ –Ω–µ—Ç", "–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å"],
        max_tabs=5,
        rate_limit_phrases=["DDoS-Guard", "DDOS", "Checking your browser", "–î–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω"],
        rate_limit_initial_delay=10.0,
        rate_limit_coef_start=1.0,
        rate_limit_coef_step=0.2,
        rate_limit_coef_max=3.0,
        handle_rate_limit=True,
        keep_browser_open=False,
        verbose=True,
        api_max_concurrent=5   # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API/–†–ì–ë –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    )

    try:
        with open("isbn_list.txt", "r", encoding="utf-8") as f:
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, —É–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
            seen = set()
            unique_isbns = []
            for line in f:
                isbn = line.strip()
                if isbn and isbn not in seen:
                    seen.add(isbn)
                    unique_isbns.append(isbn)
            isbn_list = unique_isbns
    except FileNotFoundError:
        print("–§–∞–π–ª isbn_list.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫.")
        isbn_list = [
            "978-5-907144-52-1",
            "978-0-13-417327-6",
            "978-5-04-089765-3",
            "978-5-699-93966-0"
        ]

    start = time.time()
    results = search_multiple_books(isbn_list, config)
    print(f"\n–û–±—â–µ–µ –≤—Ä–µ–º—è: {time.time() - start:.2f}—Å")
    print_results_table(isbn_list, results)