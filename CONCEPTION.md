# Обзор проекта: Извлечение ISBN из PDF

## Назначение
Проект предназначен для извлечения номеров ISBN из PDF-файлов и поиска информации о книгах с использованием этих ISBN. Он сочетает обработку PDF и веб-скрапинг для достижения своих целей.

## Основные компоненты

1. **Главный модуль (`main.py`)**:
   - Управляет общим процессом извлечения ISBN из PDF и поиска информации о книгах.
   - Использует модули `pdf_extract_isbn.py` и `web_scraper_isbn.py`.
   - Поддерживает конфигурацию через JSON-файл или аргументы командной строки.

2. **Модуль извлечения из PDF (`pdf_extract_isbn.py`)**:
   - Отвечает за извлечение ISBN из PDF-файлов.
   - Включает функции для поиска PDF-файлов и извлечения ISBN.

3. **Модули веб-скрапинга**:
   - **`web_scraper_isbn.py`** – точка входа для поиска по ISBN, использует модули `scraper.py`, `api_clients.py`, `resources.py`.
   - **`scraper.py`** – содержит основной класс `RussianBookScraperUC` для скрапинга книжных магазинов, функцию `parse_book_page_for_resource` для универсального парсинга страниц.
   - **`api_clients.py`** – асинхронные клиенты для внешних API: Google Books и Open Library.
   - **`resources.py`** – конфигурация источников данных (Читай-город, Book.ru, РГБ) с поддержкой кастомных парсеров.
   - **`config.py`** – класс `ScraperConfig` с настройками скрапинга.
   - **`drivers.py`** – утилиты для создания и управления браузером (Selenium WebDriver).
   - **`utils.py`** – вспомогательные функции, например `normalize_isbn`.

## Кэширование
- Используется для ускорения повторных запусков.
- Кэширует результаты извлечения ISBN из PDF и данные о книгах.

## Конфигурация
- Задается через файл `config.json` или аргументы командной строки.
- Включает параметры для управления процессом извлечения и скрапинга.

## Логирование
- Настроено для вывода информации о процессе выполнения и возможных ошибках.

## Резюме
Этот проект эффективно извлекает ISBN из PDF и получает информацию о книгах, используя как локальную обработку, так и онлайн-ресурсы.

## Дополнение (2026‑02‑22)
В модуль `resources.py` добавлена функция `get_resource_by_url`, которая по URL определяет, какой источник данных (Читай-город, Book.ru, РГБ) следует использовать. Это позволяет автоматически выбирать правильные селекторы при работе с тестовыми наборами данных (скрипт `debug_selectors.py`). Теперь паттерны (CSS/XPath селекторы) привязываются к ресурсу и применяются только к соответствующим URL, что исключает ошибки, когда для Book.ru использовались селекторы Читай-города.

*Обновлено: 2026‑02‑22*

## Отладка селекторов и генерация паттернов
- Скрипт `debug_selectors.py` реализует конвейер:
  1) run_parse: загрузка страницы (requests или Selenium), поиск узлов label/value (режимы `text` — по текстовым узлам, `element` — по полному тексту тега) в соответствии с параметрами exact/case_sensitive; построение HTML-фрагментов общего предка (LCA) через `html_fragment.extract_common_parent_*`.
  2) generate_pattern: генерация паттернов (CSS/XPath) для последующего извлечения значения. Сначала пытается построить CSS по `id` или локально уникальному `class`, затем формирует XPath по классам/структуре (включая сценарии соседних элементов `label -> following-sibling::value`), добавляя `resource_id` для таргетинга на конкретный сайт.
  3) run_search: верификация — для каждого URL подбирается набор паттернов по `resource_id`, загружается HTML (requests/Selenium), затем выполняется извлечение.
- Модуль `html_fragment.py` предоставляет:
  - Поиск узлов: `find_text_nodes` и `find_elements_by_text` (с нормализацией пробелов при exact=True и управлением регистром).
  - Нахождение наименьшего общего предка `lowest_common_ancestor`.
  - Извлечение фрагментов из HTML/URL/драйвера (`extract_common_parent_html`, `extract_common_parent_from_url`, `extract_common_parent_from_driver`).

Ограничения и известные риски:
- В сетевых вызовах (`requests.get`) отсутствует `timeout`, возможны зависани�� при недоступных ресурсах.
- В Selenium-режиме используется фиксированный `time.sleep(5)` вместо явных ожиданий `WebDriverWait` по условию готовности DOM.
- Генерация XPath использует `contains(text(), '...')`, что может не соответствовать нормализации текста в режиме exact и давать ложные срабатывания.
- Привязка паттернов к полям через индекс пары (в `run_search`) хрупка: порядок полей на сайте может отличаться; рекомендуется маппинг по ключам `label/value`.
- В генерации паттернов анализируется только первый фрагмент из найденных; альтернативные фрагменты игнорируются.

Намеченные улучшения:
- Добавить таймауты в сетевые вызовы и параметризов��ть их в `ScraperConfig`.
- Заменить `sleep` на `WebDriverWait` по явным условиям (например, присутствие узлов label/value).
- Нормализовать текст в XPath через `normalize-space()`/`translate()` и учитывать `exact`/`case_sensitive` при построении выражений.
- Выравнять критерии уникальности классов (в пределах `ancestor` и фрагмента) и документировать стратегию выбора атрибутов (`text`/`href`/`src`).
- Учитывать все найденные фрагменты при генерации набора паттернов.

*Обновлено: 2026‑02‑22 (расширено описание отладки селекторов)*

## Итерация 1: Исправления без изменения логики поиска (2026‑02‑22)

Выполнены следующие улучшения в рамках Итерации 1:

### 1. Исправление критичного бага в run_parse
- **Проблема**: В не-test режиме создавалась структура {url: [(label, value)]} (кортеж), но код ожидал словарь pair['label'], что вызывало TypeError.
- **Решение**: Изменён формат на {url: [{'label': label, 'value': value}]} для единообразия с test-режимом.
- **Влияние**: Скрипт debug_selectors.py теперь корректно работает в не-test режиме.

### 2. Добавление timeout в сетевые вызовы
- **Проблема**: Отсутствие таймаутов в 
equests.get могло приводить к зависаниям при недоступных ресурсах.
- **Решение**:
  - В функцию html_fragment.extract_common_parent_from_url добавлен параметр 	imeout (по умолчанию None).
  - В функции debug_selectors.run_search добавлены таймаут и использование стандартных заголовков DEFAULT_HEADERS.
  - Обработка исключений Timeout и RequestException с понятным логом.
- **Влияние**: Повышена устойчивость скрипта к сетевым проблемам.

### 3. Уточнение поведения extract_value
- **Проблема**: В ветке XPath/lxml присутствовал вводящий в заблуждение комментарий pass.
- **Решение**: Комментарий заменён на явное описание логики обработки элементов lxml.
- **Влияние**: Улучшена читаемость кода.

### 4. Исправление аннотаций типов
- **Проблема**: Неверные аннотации типов в тестовых данных и функции generate_pattern.
- **Решение**:
  - get_test_data_to_parse и get_test_data_to_search: изменён тип возвращаемого значения с dict[str, list[tuple[str, str]]] на dict[str, list[dict[str, str]]].
  - generate_pattern: исправлены типы параметра parse_frags и возвращаемого значения.
- **Влияние**: Улучшена поддержка статического анализа кода (mypy, IDE).

### Статус
Изменения Итерации 1 являются **низкорисковыми** и сохраняют обратную совместимость. Они не затрагивают основную логику поиска и генерации паттернов, а лишь исправляют ошибки и улучшают устойчивость.

## Обработка исключений при поиске отсутствующих полей (2026‑02‑22)
### Проблема
При добавлении в тестовые данных полей с несуществующими label (или value) функция `search_web` возвращает пустой список фрагментов. Это приводит к:
1. **IndexError** в `generate_pattern` при обращении к `fragments[0]`
2. Некорректному учёту пустых фрагментов в общих результатах

### Решение
1. **В `run_parse`**: Добавлена проверка `if fragments:` перед добавлением в `all_fragments`. Пустые фрагменты игнорируются с выводом предупреждения (в verbose-режиме).
2. **В `generate_pattern`**: Добавлена проверка `if not fragments:` с пропуском такого элемента и выводом предупреждения.

### Влияние
- Исключение `IndexError` больше не возникает при работе с некорректными тестовыми данными.
- Скрипт продолжает генерировать паттерны для остальных (найденных) фрагментов.
- Поведение fallback-извлечения (при пустых фрагментах, но наличии паттерна) остаётся неизменным — это фича, позволяющая извлекать значения даже если label не найден.

*Обновлено: 2026‑02‑22 (Итерация 1 завершена)*
