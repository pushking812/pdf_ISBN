# Обзор проекта: Извлечение ISBN из PDF

## Назначение
Проект предназначен для извлечения номеров ISBN из PDF-файлов и поиска информации о книгах с использованием этих ISBN. Он сочетает обработку PDF и веб-скрапинг для достижения своих целей.

## Основные компоненты

1. **Главный модуль (`main.py`)**:
   - Управляет общим процессом извлечения ISBN из PDF и поиска информации о книгах.
   - Использует модули `pdf_extract_isbn.py` и `web_scraper_isbn.py`.
   - Поддерживает конфигурацию через JSON-файл или аргументы командной строки.

2. **Модуль извлечения из PDF (`pdf_extract_isbn.py`)**:
   - Отвечает за извлечение ISBN из PDF-файлов.
   - Включает функции для поиска PDF-файлов и извлечения ISBN.

3. **Модули веб-скрапинга**:
   - **`web_scraper_isbn.py`** – точка входа для поиска по ISBN, использует модули `scraper.py`, `api_clients.py`, `resources.py`.
   - **`scraper.py`** – содержит основной класс `RussianBookScraperUC` для скрапинга книжных магазинов, функцию `parse_book_page_for_resource` для универсального парсинга страниц.
   - **`api_clients.py`** – асинхронные клиенты для внешних API: Google Books и Open Library.
   - **`resources.py`** – конфигурация источников данных (Читай-город, Book.ru, РГБ) с поддержкой кастомных парсеров.
   - **`config.py`** – класс `ScraperConfig` с настройками скрапинга.
   - **`drivers.py`** – утилиты для создания и управления браузером (Selenium WebDriver).
   - **`utils.py`** – вспомогательные функции, например `normalize_isbn`.

## Кэширование
- Используется для ускорения повторных запусков.
- Кэширует результаты извлечения ISBN из PDF и данные о книгах.

## Конфигурация
- Задается через файл `config.json` или аргументы командной строки.
- Включает параметры для управления процессом извлечения и скрапинга.

## Логирование
- Настроено для вывода информации о процессе выполнения и возможных ошибках.

## Резюме
Этот проект эффективно извлекает ISBN из PDF и получает информацию о книгах, используя как локальную обработку, так и онлайн-ресурсы.

## Дополнение (2026‑02‑22)
В модуль `resources.py` добавлена функция `get_resource_by_url`, которая по URL определяет, какой источник данных (Читай-город, Book.ru, РГБ) следует использовать. Это позволяет автоматически выбирать правильные селекторы при работе с тестовыми наборами данных (скрипт `debug_selectors.py`). Теперь паттерны (CSS/XPath селекторы) привязываются к ресурсу и применяются только к соответствующим URL, что исключает ошибки, когда для Book.ru использовались селекторы Читай-города.

*Обновлено: 2026‑02‑22*

## Отладка селекторов и генерация паттернов
- Скрипт `debug_selectors.py` реализует конвейер:
  1) run_parse: загрузка страницы (requests или Selenium), поиск узлов label/value (режимы `text` — по текстовым узлам, `element` — по полному тексту тега) в соответствии с параметрами exact/case_sensitive; построение HTML-фрагментов общего предка (LCA) через `html_fragment.extract_common_parent_*`.
  2) generate_pattern: генерация паттернов (CSS/XPath) для последующего извлечения значения. Сначала пытается построить CSS по `id` или локально уникальному `class`, затем формирует XPath по классам/структуре (включая сценарии соседних элементов `label -> following-sibling::value`), добавляя `resource_id` для таргетинга на конкретный сайт.
  3) run_search: верификация — для каждого URL подбирается набор паттернов по `resource_id`, загружается HTML (requests/Selenium), затем выполняется извлечение.
- Модуль `html_fragment.py` предоставляет:
  - Поиск узлов: `find_text_nodes` и `find_elements_by_text` (с нормализацией пробелов при exact=True и управлением регистром).
  - Нахождение наименьшего общего предка `lowest_common_ancestor`.
  - Извлечение фрагментов из HTML/URL/драйвера (`extract_common_parent_html`, `extract_common_parent_from_url`, `extract_common_parent_from_driver`).

Ограничения и известные риски:
- В сетевых вызовах (`requests.get`) отсутствует `timeout`, возможны зависани�� при недоступных ресурсах.
- В Selenium-режиме используется фиксированный `time.sleep(5)` вместо явных ожиданий `WebDriverWait` по условию готовности DOM.
- Генерация XPath использует `contains(text(), '...')`, что может не соответствовать нормализации текста в режиме exact и давать ложные срабатывания.
- Привязка паттернов к полям через индекс пары (в `run_search`) хрупка: порядок полей на сайте может отличаться; рекомендуется маппинг по ключам `label/value`.
- В генерации паттернов анализируется только первый фрагмент из найденных; альтернативные фрагменты игнорируются.

Намеченные улучшения:
- Добавить таймауты в сетевые вызовы и параметризов��ть их в `ScraperConfig`.
- Заменить `sleep` на `WebDriverWait` по явным условиям (например, присутствие узлов label/value).
- Нормализовать текст в XPath через `normalize-space()`/`translate()` и учитывать `exact`/`case_sensitive` при построении выражений.
- Выравнять критерии уникальности классов (в пределах `ancestor` и фрагмента) и документировать стратегию выбора атрибутов (`text`/`href`/`src`).
- Учитывать все найденные фрагменты при генерации набора паттернов.

*Обновлено: 2026‑02‑22 (расширено описание отладки селекторов)*