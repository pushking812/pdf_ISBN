#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—è html_fragment.

–ü–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –ø–æ–ª—É—á–∏—Ç—å HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–∞—Ä—É ¬´–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è ‚Äì –∑–Ω–∞—á–µ–Ω–∏–µ¬ª,
—á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤ —Ä—É—á–Ω–æ–º –ø–æ–¥–±–æ—Ä–µ CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python debug_selectors.py <URL> <label_text> <value_text> [--selenium] [--exact] [--case-sensitive]

–ü—Ä–∏–º–µ—Ä:
    python debug_selectors.py https://example.com/book "–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è" "2020"
"""

import sys
import argparse
from typing import Optional
from selenium.webdriver.remote.webdriver import WebDriver

from html_fragment import (
    extract_common_parent_from_url,
    extract_common_parent_from_driver,
    find_elements_by_text,
    find_text_nodes,
    lowest_common_ancestor,
)
from bs4 import BeautifulSoup, Tag
from typing import Dict, Any, Union


def parse_arguments(
    url: str, 
    label: str, 
    value: str, 
    selenium: bool,
    exact: bool, 
    verbose: bool,
    test: bool,
    search_mode: str,
    case_sensitive: bool = False,
    all_matches: bool = False,
    ) -> argparse.Namespace:
    """–ü–∞—Ä—Å–∏—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –Ω–∏–º–∏."""
    parser = argparse.ArgumentParser(
        description="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –ø–∞—Ä–µ ¬´–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è ‚Äì –∑–Ω–∞—á–µ–Ω–∏–µ¬ª."
    )
    parser.add_argument(
        "url",
        help="URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
        nargs='?',
        default=url,
        )
    parser.add_argument(
        "label",
        help="–¢–µ–∫—Å—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è')",
        nargs='?',
        default = label,
        )
    parser.add_argument(
        "value",
        help="–¢–µ–∫—Å—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '2020')",
        nargs='?',
        default = value,
        )
    parser.add_argument(
        "--selenium",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Selenium WebDriver (–¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)",
        default = selenium,
    )
    parser.add_argument(
        "--exact",
        action="store_true",
        help="–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äì —á–∞—Å—Ç–∏—á–Ω–æ–µ)",
        default=exact,
    )
    parser.add_argument(
        "--case-sensitive",
        action="store_true",
        help="–£—á–∏—Ç—ã–≤–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äì –Ω–µ—Ç)",
        default = case_sensitive,
    )
    parser.add_argument(
        "--all-matches",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äì —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π)",
        default = all_matches,
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–í—ã–≤–æ–¥–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
        default=verbose,
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (–∂—ë—Å—Ç–∫–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL –∏ –ø–∞—Ä—ã)",
        default=test,
    )
    parser.add_argument(
        "--search-mode",
        choices=["text", "element"],
        default=search_mode,
        help="–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ —É–∑–ª–æ–≤: text (–ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —É–∑–ª–∞–º), element (–ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º)",
    )
    return parser.parse_args()


def get_test_data_to_parse() -> dict[str, list[tuple[str, str]]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (URL -> —Å–ø–∏—Å–æ–∫ –ø–∞—Ä label-value)."""
    return {
        "https://book.ru/book/943665": [
            {'label':'–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è:', 'value': '2022'},
            {'label':'–ê–≤—Ç–æ—Ä—ã:', 'value': '–ö—Ä–∏–≤–æ–ª–∞–ø–æ–≤ –°.–Ø., –•—Ä–∏–ø—É–Ω–æ–≤–∞ –ú.–ë.'},
            {'label':'–û–±—ä–µ–º:', 'value': '455 —Å—Ç—Ä.'},
        ],
    }
    
def get_test_data_to_search() -> dict[str, list[tuple[str, str]]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (URL -> —Å–ø–∏—Å–æ–∫ –ø–∞—Ä label-value)."""
    return {
        "https://book.ru/book/962004":
            [{'label':'–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è:', 'value': '2026'},
             {'label':'–ê–≤—Ç–æ—Ä—ã:', 'value': '–ü–∞—Ä—à–∏–Ω—Ü–µ–≤–∞ –õ.–°., –ü–∞—Ä—à–∏–Ω—Ü–µ–≤ –ê.–ê.'},
             {'label':'–û–±—ä–µ–º:', 'value': '129 —Å—Ç—Ä.'}],
    }


def create_driver(headless: bool = False) -> WebDriver:
    """–°–æ–∑–¥–∞—ë—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä ChromeDriver."""
    from drivers import create_chrome_driver
    from config import ScraperConfig
    config = ScraperConfig(headless=headless)
    return create_chrome_driver(config)


def search_web(
    url: str,
    is_driver: bool,
    label: str,
    value: str,
    exact_label: bool,
    exact_value: bool,
    case_sensitive: bool,
    all_matches: bool,
    verbose: bool,
    search_mode: str,
    driver: Optional[WebDriver] = None,
) -> list[str]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium (–µ—Å–ª–∏ is_driver=True) –∏–ª–∏ requests.
    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω driver, –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è Selenium-–ø–æ–∏—Å–∫–∞ (is_driver –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å True).
    –ï—Å–ª–∏ driver –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –∏ is_driver=True, —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π –¥—Ä–∞–π–≤–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞.
    """
    func = extract_common_parent_from_url
    driver_or_url = url
    created_driver = None

    if is_driver:
        if driver is not None:
            driver_or_url = driver
        else:
            created_driver = create_driver(headless=False)
            driver_or_url = created_driver
            driver_or_url.get(url)
        func = extract_common_parent_from_driver
    else:
        # –ï—Å–ª–∏ is_driver=False, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π driver (–æ–Ω –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞–Ω)
        pass

    try:
        return func(
            driver_or_url,
            label,
            value,
            exact_label=exact_label,
            exact_value=exact_value,
            case_sensitive=case_sensitive,
            all_matches=all_matches,
            verbose=verbose,
            search_mode=search_mode,
        )
    finally:
        if created_driver is not None:
            created_driver.quit()


def generate_pattern(
    parse_frags: str,
    args: argparse.Namespace,
) -> Dict[str, Any]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –∏–ª–∏ XPath) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è
    –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É HTML, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º—É –ø–∞—Ä—É ¬´–∫–ª—é—á–µ–≤–æ–µ –ø–æ–ª–µ ‚Äì –∑–Ω–∞—á–µ–Ω–∏–µ¬ª.
    """
    search_mode: str = args.search_mode
    exact_label: bool = args.exact
    exact_value: bool = args.exact
    case_sensitive: bool = args.case_sensitive
    
    patterns = []
    
    for parse_frag in parse_frags:
        print(f"\n=== –§—Ä–∞–≥–º–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ ===")
        print(parse_frag)
        print("=" * 50)
    
        label_text: str= parse_frag[1] # label
        value_text: str= parse_frag[2] # value
    
        soup = BeautifulSoup(parse_frag[3][0], "lxml") # html —Ñ—Ä–∞–≥–º–µ–Ω—Ç
    
        # –ù–∞—Ö–æ–¥–∏–º —É–∑–ª—ã label –∏ value
        if search_mode == "text":
            label_nodes = find_text_nodes(soup, label_text, exact=exact_label, case_sensitive=case_sensitive)
            value_nodes = find_text_nodes(soup, value_text, exact=exact_value, case_sensitive=case_sensitive)
        else:
            label_nodes = find_elements_by_text(soup, label_text, exact=exact_label, case_sensitive=case_sensitive)
            value_nodes = find_elements_by_text(soup, value_text, exact=exact_value, case_sensitive=case_sensitive)
    
        if not label_nodes or not value_nodes:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ label –∏–ª–∏ value –≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ")
    
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –ø–æ–ø–∞–≤—à–∏–µ—Å—è —É–∑–ª—ã (–≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–¥–Ω–∞ –ø–∞—Ä–∞)
        label_node = label_nodes[0]
        value_node = value_nodes[0]
    
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–µ–≥–æ –ø—Ä–µ–¥–∫–∞ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –Ω–æ –≤—ã—á–∏—Å–ª–∏–º –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏)
        ancestor = lowest_common_ancestor(label_node, value_node)
        if ancestor is None:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ–±—â–µ–≥–æ –ø—Ä–µ–¥–∫–∞ –¥–ª—è label –∏ value")
    
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        attribute = "text"
        if isinstance(value_node, Tag):
            if value_node.name == "a":
                attribute = "href"
            elif value_node.has_attr("src"):
                attribute = "src"
            elif value_node.has_attr("content"):
                attribute = "content"
    
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É –∏–ª–∏ id
        def get_css_selector(element: Tag) -> str:
            # –ï—Å–ª–∏ –µ—Å—Ç—å id ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if element.has_attr("id"):
                return f"#{element['id']}"
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞)
            if element.has_attr("class"):
                classes = element["class"]
                if isinstance(classes, list):
                    for cls in classes:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç –∫–ª–∞—Å—Å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —É –¥–∞–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ soup
                        if len(soup.select(f".{cls}")) == 1:
                            return f".{cls}"
            # –ò–Ω–∞—á–µ —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ —Ç–µ–≥—É —Å —É—á—ë—Ç–æ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
            # –ü–æ–∫–∞ –≤–µ—Ä–Ω—ë–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ XPath
            return ""
    
        css_selector = ""
        if isinstance(value_node, Tag):
            css_selector = get_css_selector(value_node)
    
        if css_selector:
            # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —Å–µ–ª–µ–∫—Ç–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç value –≤–Ω—É—Ç—Ä–∏ ancestor
            # (–ø—Ä–æ–ø—É—Å—Ç–∏–º —Å–ª–æ–∂–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É)
            pattern = {
                "type": "css",
                "selector": css_selector,
                "attribute": attribute,
                "label_text": label_text,
                "value_text": value_text,
                "clean_regex": None,
            }
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º XPath —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç –∑–Ω–∞—á–µ–Ω–∏—è (—Ç–µ–≥)
            value_element = value_node if isinstance(value_node, Tag) else value_node.parent
            # –°–æ–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å—ã —ç–ª–µ–º–µ–Ω—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏—è
            value_classes = []
            if isinstance(value_element, Tag) and value_element.has_attr("class"):
                classes = value_element["class"]
                if isinstance(classes, list):
                    value_classes = classes
            # –°–æ–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å—ã –ø—Ä–µ–¥–∫–∞
            ancestor_classes = []
            if ancestor.has_attr("class"):
                classes = ancestor["class"]
                if isinstance(classes, list):
                    ancestor_classes = classes
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–æ—á–Ω—ã–π XPath
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –µ—Å–ª–∏ —É –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø—Ä–µ–¥–∫–∞
            selected_class = None
            for cls in value_classes:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç –∫–ª–∞—Å—Å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –≤–Ω—É—Ç—Ä–∏ ancestor
                if len(ancestor.select(f".{cls}")) == 1:
                    selected_class = cls
                    break
            
            if selected_class:
                # XPath –ø–æ –∫–ª–∞—Å—Å—É –∑–Ω–∞—á–µ–Ω–∏—è —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ label
                xpath = f"//*[contains(@class, '{selected_class}')]"
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: ancestor —Å –∫–ª–∞—Å—Å–æ–º + label —Ç–µ–∫—Å—Ç + –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —Ç–µ–≥—É
                ancestor_class_part = ""
                if ancestor_classes:
                    ancestor_class_part = f"[contains(@class, '{ancestor_classes[0]}')]"
                value_tag = value_element.name if isinstance(value_element, Tag) else "*"
                xpath = f"//*{ancestor_class_part}[.//*[contains(text(), '{label_text}')]]//{value_tag}"
            
            pattern = {
                "type": "xpath",
                "selector": xpath,
                "attribute": attribute,
                "label_text": label_text,
                "value_text": value_text,
                "clean_regex": None,
            }
            
        print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern['type']} -> {pattern['selector']} (–∞—Ç—Ä–∏–±—É—Ç: {pattern['attribute']})")
        
        patterns.append(pattern)
        
    return patterns


def extract_value(
    html_or_driver: Union[str, WebDriver],
    pattern: Dict[str, Any],
    use_selenium: Optional[bool] = None,
) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –∏–∑ HTML –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Selenium –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É.
    """
    from selenium.webdriver.common.by import By

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Ä–∞–±–æ—Ç–∞–µ–º –ª–∏ —Å Selenium
    is_selenium = use_selenium if use_selenium is not None else isinstance(html_or_driver, WebDriver)
    
    if is_selenium:
        driver = html_or_driver
        selector = pattern["selector"]
        selector_type = pattern["type"]
        
        try:
            if selector_type == "css":
                element = driver.find_element(By.CSS_SELECTOR, selector)
            elif selector_type == "xpath":
                element = driver.find_element(By.XPATH, selector)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {selector_type}")
        except Exception:
            # –≠–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
            return None
    else:
        html = html_or_driver
        soup = BeautifulSoup(html, "lxml")
        selector = pattern["selector"]
        selector_type = pattern["type"]
        
        if selector_type == "css":
            element = soup.select_one(selector)
            if element is None:
                return None
        elif selector_type == "xpath":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º lxml –¥–ª—è XPath
            from lxml import etree
            tree = etree.HTML(html)
            try:
                elements = tree.xpath(selector)
                if not elements:
                    return None
                element = elements[0]
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–ª–µ–º–µ–Ω—Ç lxml –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤/—Ç–µ–∫—Å—Ç–∞
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å lxml.etree._Element
                pass
            except Exception:
                return None
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {selector_type}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞
    attribute = pattern.get("attribute", "text")
    
    if is_selenium:
        if attribute == "text":
            value = element.text
        else:
            value = element.get_attribute(attribute)
    else:
        # BeautifulSoup –∏–ª–∏ lxml —ç–ª–µ–º–µ–Ω—Ç
        if selector_type == "css":
            if attribute == "text":
                value = element.get_text(strip=True)
            else:
                value = element.get(attribute)
        else:  # xpath —Å lxml
            if attribute == "text":
                value = element.text
            else:
                value = element.get(attribute)
    
    if value is None:
        return None
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º clean_regex, –µ—Å–ª–∏ –µ—Å—Ç—å
    clean_regex = pattern.get("clean_regex")
    if clean_regex and value:
        import re
        match = re.search(clean_regex, value)
        if match:
            value = match.group(1) if match.groups() else match.group(0)
    
    return value.strip() if isinstance(value, str) else str(value)


def print_fragments(fragments: list[tuple]) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª—å."""
    if not fragments:
        print("‚ùå –§—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(fragments)}")
    for i, frag in enumerate(fragments, 1):
        print(f"\n=== –§—Ä–∞–≥–º–µ–Ω—Ç {i} ===")
        print(f"URL: {frag[0]}, Label: '{frag[1]}', Value: '{frag[2]}'")
        print("-" * 50)
        print(frag[3])
        print("=" * 50)


def run_parse(args: argparse.Namespace) -> Union[bool, list[str]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω, –∏–Ω–∞—á–µ False.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
    if args.test:
        search_data = get_test_data_to_parse()
    else:
        search_data = {
            args.url: [(args.label, args.value)]
        }

    all_fragments = []
    driver = create_driver(headless=False) if args.selenium else None
    try:
        for url, pairs in search_data.items():
            if args.verbose:
                print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ URL: {url}")

            if driver:
                driver.get(url)

            
            for pair in pairs:
                if args.verbose:
                    print(f"\n=== –ü–æ–∏—Å–∫ –ø–∞—Ä—ã: '{pair['label']}' ‚Äì '{pair['value']}' ===")

                fragments = search_web(
                    url,
                    is_driver=args.selenium,
                    label=pair['label'],
                    value=pair['value'],
                    exact_label=args.exact,
                    exact_value=args.exact,
                    case_sensitive=args.case_sensitive,
                    all_matches=args.all_matches,
                    verbose=args.verbose,
                    search_mode=args.search_mode,
                    driver=driver,
                )
                all_fragments.extend([(url, pair['label'], pair['value'], fragments)])
    finally:
        if driver:
            driver.quit()

    if not all_fragments:
        print("‚ùå –§—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        if not args.verbose:
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º --verbose, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.")
        return False

    print_fragments(all_fragments) # html —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–∞—Ä—ã (–¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏)
    return all_fragments


def run_search(args, patterns) -> list[Optional[str]]:
    search_data = get_test_data_to_search()
    all_extracted = []
    
    for url, pairs in search_data.items():
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ URL: {url} —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º '{patterns[0]['type']}'")
        
        for pair in pairs:
            print(f"\n=== –ü–æ–∏—Å–∫ –ø–∞—Ä—ã: '{pair['label']}' ‚Äì '{pair['value']}' ===")
            
            search_frags = extract_common_parent_from_url(
                                    url=url,
                                    label_text=pair['label'],  # label
                                    value_text=pair['value'],  # value
                                    exact_label=args.exact,
                                    exact_value=args.exact,
                                    case_sensitive=args.case_sensitive,
                                    all_matches=args.all_matches,
                                    verbose=args.verbose,
                                    search_mode=args.search_mode,
                                )
            
            extracted = extract_value(search_frags[0], patterns[0])
            
            print(f"–ò–∑–≤–ª–µ—á—ë–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {extracted}")
    
            all_extracted.append(extracted)
    
    return all_extracted
    
def main() -> None:
    default_arg_values = {
        "url": r"https://book.ru/book/943665",
        "label": "–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è:",
        "value": "2022",
        "selenium": False,
        "exact": True,
        "verbose": True,
        "test": True,
        "search_mode": "element",
        "all_matches": True,
    }
    args = parse_arguments(**default_arg_values)
    
    parse_frags = run_parse(args)
    if not parse_frags:
        print("   ‚ùå –§—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        sys.exit(1)
        
    patterns = generate_pattern(
        parse_frags,
        args = args
    )

    # for pattern in patterns:
    #     print(f"   –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {pattern['type']}")
    #     print(f"   –°–µ–ª–µ–∫—Ç–æ—Ä: {pattern['selector']}")
    
    values = run_search(args, patterns)

if __name__ == "__main__":
    main()
