#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN –∏–∑ PDF-—Ñ–∞–π–ª–æ–≤ –∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- pdf_extract_isbn.py –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN –∏–∑ PDF
- web_scraper_isbn.py –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ ISBN (API –∏ —Å–∫—Ä–∞–ø–∏–Ω–≥)

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ JSON-—Ñ–∞–π–ª (–ø–∞—Ä–∞–º–µ—Ç—Ä --config) –∏–ª–∏ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
(—Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, headless, verbose, output). –î–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ JSON.
"""

import asyncio
import argparse
import json
import logging
import sys
import time
import random
from typing import Dict, List, Optional, Tuple, Any, Callable
from collections import defaultdict
from functools import partial
import os

# –ò–º–ø–æ—Ä—Ç –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from pdf_extract_isbn import scan_pdfs, logger as pdf_logger
from web_scraper_isbn import (
    search_multiple_books,
    ScraperConfig,
    normalize_isbn,
    run_api_stage,
    RussianBookScraperUC,
    TabState,
    TabInfo,
    TimeoutException,
    NoSuchElementException,
    By,
    WebDriverWait,
    EC,
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("main")

# ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==========

def truncate_path(path: str, max_len: int = 60) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–µ—Ü, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π."""
    if len(path) <= max_len:
        return path
    return "..." + path[-(max_len - 3):]


def load_config_from_json(json_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON-—Ñ–∞–π–ª–∞."""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def merge_config(base_config: dict, cli_args: dict) -> dict:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã CLI –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç.
    """
    merged = base_config.copy()
    for key, value in cli_args.items():
        if value is not None:  # —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç —è–≤–Ω–æ –∑–∞–¥–∞–Ω
            merged[key] = value
    return merged


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –≠–¢–ê–ü–ê –°–ö–†–ê–ü–ò–ù–ì–ê –° –¢–ê–ë–õ–ò–ß–ù–´–ú –ü–†–û–ì–†–ï–°–°–û–ú ==========

def parallel_search_with_progress(
    isbn_list: List[str],
    config: ScraperConfig,
    progress_callback: Callable[[int, Optional[Dict[str, Any]]], None]
) -> List[Optional[Dict[str, Any]]]:
    """
    –ê–Ω–∞–ª–æ–≥ async_parallel_search –∏–∑ web_scraper_isbn, –Ω–æ —Å –≤—ã–∑–æ–≤–æ–º callback
    –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ ISBN (–ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏).
    progress_callback(index, result) –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –¥–ª—è ISBN —Å –¥–∞–Ω–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
    –ø–æ–ª—É—á–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω).
    """
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º undetected_chromedriver –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ª–∏—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –º–æ–¥—É–ª–µ
    import undetected_chromedriver as uc

    # –°–æ–∑–¥–∞—ë–º –¥—Ä–∞–π–≤–µ—Ä (–≤–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞, —á—Ç–æ–±—ã —É–ø—Ä–∞–≤–ª—è—Ç—å –≤—Ä—É—á–Ω—É—é)
    driver = uc.Chrome(headless=config.headless)
    driver.set_window_size(1920, 1080)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not config.skip_main_page:
        driver.get(config.base_url)
        time.sleep(random.uniform(*config.delay_after_main))
        try:
            WebDriverWait(driver, config.wait_city_modal).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), '–î–∞, —è –∑–¥–µ—Å—å')]"))
            ).click()
            if config.verbose:
                logger.debug("–ì–æ—Ä–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω (–≥–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ)")
            time.sleep(random.uniform(*config.delay_between_actions))
        except:
            pass
    else:
        if config.verbose:
            logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ ISBN –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ max_tabs
    chunks = [isbn_list[i:i + config.max_tabs] for i in range(0, len(isbn_list), config.max_tabs)]
    all_results = [None] * len(isbn_list)
    rate_limit_attempts = 0

    # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç scraper –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
    scraper_template = RussianBookScraperUC(config)

    for chunk_idx, chunk in enumerate(chunks):
        if config.verbose:
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {chunk_idx + 1}/{len(chunks)} (ISBN: {chunk})")

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
        handles = []
        try:
            main_handle = driver.current_window_handle
            handles.append(main_handle)
            for i in range(1, len(chunk)):
                driver.switch_to.new_window('tab')
                time.sleep(0.5)
                new_handle = driver.current_window_handle
                if new_handle not in handles:
                    handles.append(new_handle)
                else:
                    all_handles = driver.window_handles
                    found = False
                    for h in all_handles:
                        if h not in handles:
                            handles.append(h)
                            found = True
                            break
                    if not found:
                        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–π handle –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ {i}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_idx}: {e}")
            # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ ISBN —á–∞–Ω–∫–∞ –∫–∞–∫ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            for j, _ in enumerate(chunk):
                idx = chunk_idx * config.max_tabs + j
                all_results[idx] = None
                progress_callback(idx, None)
            time.sleep(1)
            continue

        # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç—ã TabInfo —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
        tabs = []
        for i, isbn in enumerate(chunk):
            global_idx = chunk_idx * config.max_tabs + i
            tabs.append(TabInfo(isbn, handles[i], global_idx, config))

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞–∂–¥—É—é –≤–∫–ª–∞–¥–∫—É
        for tab in tabs:
            try:
                driver.switch_to.window(tab.handle)
                clean_isbn = tab.isbn.replace("-", "").strip()
                search_url = f"{config.base_url}/search?phrase={clean_isbn}"
                driver.get(search_url)
                tab.state = TabState.SEARCHING
                tab.search_start_time = time.time()
                time.sleep(0.2)
            except Exception as e:
                logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–∏—Å–∫: {e}")
                tab.state = TabState.ERROR
                progress_callback(tab.index, None)

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞
        all_done = False
        while not all_done:
            all_done = True
            for tab in tabs:
                if tab.state in (TabState.DONE, TabState.ERROR, TabState.RATE_LIMITED):
                    continue
                all_done = False

                try:
                    driver.switch_to.window(tab.handle)
                except Exception as e:
                    logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è: {e}")
                    tab.state = TabState.ERROR
                    progress_callback(tab.index, None)
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if config.handle_rate_limit:
                    try:
                        page_source = driver.page_source
                        found_rate_limit = any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases)
                        if found_rate_limit:
                            logger.warning(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                            rate_limit_attempts += 1
                            coef = config.rate_limit_coef_start + (rate_limit_attempts - 1) * config.rate_limit_coef_step
                            coef = min(coef, config.rate_limit_coef_max)
                            wait_time = config.rate_limit_initial_delay * coef
                            logger.info(f"–ü–∞—É–∑–∞ {wait_time:.1f}—Å (–∫–æ—ç—Ñ. {coef:.2f})")
                            time.sleep(wait_time)
                            driver.refresh()
                            while True:
                                time.sleep(config.poll_interval)
                                page_source = driver.page_source
                                if any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases):
                                    time.sleep(wait_time)
                                    driver.refresh()
                                else:
                                    logger.debug("–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–Ω—è—Ç–∞, —Å–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞")
                                    rate_limit_attempts = 0
                                    break
                            for t in tabs:
                                if t.handle != tab.handle:
                                    try:
                                        driver.switch_to.window(t.handle)
                                        driver.refresh()
                                        time.sleep(0.5)
                                    except:
                                        pass
                            driver.switch_to.window(tab.handle)
                            break
                        else:
                            if rate_limit_attempts > 0:
                                rate_limit_attempts = 0
                    except Exception:
                        pass

                if tab.state == TabState.SEARCHING:
                    try:
                        page_source = driver.page_source
                        if any(phrase in page_source for phrase in config.no_product_phrases):
                            logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
                            tab.state = TabState.ERROR
                            progress_callback(tab.index, None)
                            continue
                    except:
                        pass

                    elapsed = time.time() - tab.search_start_time
                    found_link = None
                    for selector in scraper_template.product_link_selectors:
                        try:
                            element = driver.find_element(By.CSS_SELECTOR, selector)
                            found_link = element
                            break
                        except NoSuchElementException:
                            continue
                    if found_link:
                        tab.book_url = found_link.get_attribute('href')
                        driver.get(tab.book_url)
                        time.sleep(random.uniform(*config.delay_after_click))
                        tab.state = TabState.BOOK_PAGE
                        logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏")
                    else:
                        if elapsed > tab.timeout:
                            logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç")
                            tab.state = TabState.ERROR
                            progress_callback(tab.index, None)

                elif tab.state == TabState.BOOK_PAGE:
                    try:
                        scraper_template.driver = driver
                        result = scraper_template._parse_book_page()
                        tab.result = result
                        tab.state = TabState.DONE
                        rate_limit_attempts = 0
                        all_results[tab.index] = result
                        progress_callback(tab.index, result)
                        logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] ISBN {tab.isbn} –≥–æ—Ç–æ–≤")
                    except Exception as e:
                        logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                        tab.state = TabState.ERROR
                        progress_callback(tab.index, None)

            if not all_done:
                time.sleep(config.poll_interval)

        # –ó–∞–∫—Ä—ã—Ç–∏–µ –≤–∫–ª–∞–¥–æ–∫ —á–∞–Ω–∫–∞ (–∫—Ä–æ–º–µ –≥–ª–∞–≤–Ω–æ–π)
        main_handle = handles[0]
        for handle in handles[1:]:
            try:
                driver.switch_to.window(handle)
                driver.close()
            except:
                pass
        driver.switch_to.window(main_handle)
        time.sleep(1)

    if not config.keep_browser_open:
        driver.quit()
    else:
        input("\nüîç –ë—Ä–∞—É–∑–µ—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–º. –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è...")
        driver.quit()

    return all_results


async def run_scraping_stage(
    isbn_list: List[str],
    indices: List[int],  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    config: ScraperConfig
) -> List[Optional[Dict[str, Any]]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∫—Ä–∞–ø–∏–Ω–≥ –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥–∞ –¥–ª—è —Å–ø–∏—Å–∫–∞ ISBN —Å –≤—ã–≤–æ–¥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ç–∞–±–ª–∏—Ü—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä —Ä–∞–≤–µ–Ω len(isbn_list)) –≤ –ø–æ—Ä—è–¥–∫–µ isbn_list.
    """
    if not isbn_list:
        return []

    print(f"\nüîç –°–∫—Ä–∞–ø–∏–Ω–≥ –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥ –¥–ª—è {len(isbn_list)} ISBN (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ):")
    header = f"{' ‚Ññ':>4} | {'ISBN':<20} | {'–°—Ç–∞—Ç—É—Å':<25} | {'–ù–∞–∑–≤–∞–Ω–∏–µ'}"
    print(header)
    print("-" * len(header))

    results = [None] * len(isbn_list)

    def progress_callback(idx: int, res: Optional[Dict[str, Any]]):
        # idx - –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ isbn_list (0..len(isbn_list)-1)
        isbn = isbn_list[idx]
        if res:
            title = res.get('title', '')[:47] + '...' if len(res.get('title', '')) > 50 else res.get('title', '')
            status = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ ({res.get('source', '–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥')})"
            print(f"{idx+1:4} | {isbn:<20} | {status:<25} | {title}")
        else:
            print(f"{idx+1:4} | {isbn:<20} | {'‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ':<25} |")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é parallel_search_with_progress –≤ executor
    loop = asyncio.get_running_loop()
    scraped_results = await loop.run_in_executor(
        None,
        parallel_search_with_progress,
        isbn_list,
        config,
        progress_callback
    )
    print("-" * len(header))
    return scraped_results


# ========== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ==========

async def collect_isbns_from_pdfs(
    directory: str,
    max_workers: Optional[int] = None,
    strict: bool = True,
    include_metadata: bool = False,
    max_pages: int = 10,
    max_concurrent: Optional[int] = None,
) -> List[Tuple[str, Optional[str], str]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –∏–∑–≤–ª–µ–∫–∞–µ—Ç ISBN –∏–∑ PDF-—Ñ–∞–π–ª–æ–≤ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Å–µ–º–∞—Ñ–æ—Ä—É)."""
    results = []
    logger.info(f"–ü–æ–∏—Å–∫ PDF –≤ {directory}...")
    async for pdf_path, isbn, source in scan_pdfs(
        directory=directory,
        max_workers=max_workers,
        strict=strict,
        include_metadata=include_metadata,
        max_pages=max_pages,
        max_concurrent=max_concurrent,
    ):
        results.append((pdf_path, isbn, source))
        if isbn:
            logger.debug(f"[{pdf_path}] -> ISBN: {isbn} ({source})")
        else:
            logger.debug(f"[{pdf_path}] -> ISBN –Ω–µ –Ω–∞–π–¥–µ–Ω")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ PDF: {len(results)}")
    return results


def build_isbn_mapping(
    pdf_results: List[Tuple[str, Optional[str], str]]
) -> Tuple[Dict[str, List[Tuple[str, str, str]]], List[str]]:
    """
    –°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å: isbn -> —Å–ø–∏—Å–æ–∫ (–ø—É—Ç—å_–∫_pdf, –∏—Å—Ç–æ—á–Ω–∏–∫, isbn_raw).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö ISBN.
    """
    mapping = defaultdict(list)
    unique_isbns = []
    for pdf_path, isbn, source in pdf_results:
        if isbn:
            norm_isbn = normalize_isbn(isbn)
            if norm_isbn:
                mapping[norm_isbn].append((pdf_path, source, isbn))
                if norm_isbn not in unique_isbns:
                    unique_isbns.append(norm_isbn)
            else:
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ISBN {isbn} –≤ —Ñ–∞–π–ª–µ {pdf_path} –ø—Ä–æ–ø—É—â–µ–Ω")
    return mapping, unique_isbns


def print_pdf_results_table(
    pdf_results: List[Tuple[str, Optional[str], str]],
    book_data: Dict[str, Optional[Dict[str, Any]]]
) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ PDF."""
    print("\n" + "=" * 140)
    header = (
        f"{'PDF —Ñ–∞–π–ª':<60} {'ISBN':<20} {'–ò—Å—Ç–æ—á–Ω–∏–∫':<10} "
        f"{'–ù–∞–∑–≤–∞–Ω–∏–µ':<40} {'–ê–≤—Ç–æ—Ä(—ã)':<30} {'–°—Ç—Ä.':<6} {'–ì–æ–¥':<5}"
    )
    print(header)
    print("=" * 140)

    for pdf_path, isbn, src in pdf_results:
        display_path = truncate_path(pdf_path, 60)

        if not isbn:
            print(f"{display_path:<60} {'‚Äî':<20} {'‚Äî':<10} {'‚ùå ISBN –Ω–µ –Ω–∞–π–¥–µ–Ω':<40} {'':<30} {'':<6} {'':<5}")
            continue

        norm_isbn = normalize_isbn(isbn)
        if not norm_isbn:
            print(f"{display_path:<60} {isbn:<20} {src:<10} {'‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ISBN':<40} {'':<30} {'':<6} {'':<5}")
            continue

        data = book_data.get(norm_isbn)
        if not data:
            print(f"{display_path:<60} {isbn:<20} {src:<10} {'‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞':<40} {'':<30} {'':<6} {'':<5}")
            continue

        # –£—Å–µ–∫–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        title = data.get('title', '')[:37] + '...' if len(data.get('title', '')) > 40 else data.get('title', '')
        authors = ', '.join(data.get('authors', []))[:27] + '...' if len(', '.join(data.get('authors', []))) > 30 else ', '.join(data.get('authors', []))
        pages = data.get('pages', '‚Äî')
        year = data.get('year', '‚Äî')
        source_web = data.get('source', '‚Äî')

        print(
            f"{display_path:<60} {isbn:<20} {src:<10} "
            f"{title:<40} {authors:<30} {pages:<6} {year:<5} "
        )
    print("=" * 140)


def save_results_to_json(
    pdf_results: List[Tuple[str, Optional[str], str]],
    book_data: Dict[str, Optional[Dict[str, Any]]],
    output_file: str
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Ñ–∞–π–ª."""
    output = []
    for pdf_path, isbn, src in pdf_results:
        record = {
            "pdf_path": pdf_path,
            "isbn_raw": isbn,
            "source_pdf": src,
        }
        if isbn:
            norm_isbn = normalize_isbn(isbn)
            if norm_isbn and norm_isbn in book_data:
                record["book_info"] = book_data[norm_isbn]
            else:
                record["book_info"] = None
        else:
            record["book_info"] = None
        output.append(record)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


async def async_main(args):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""

    # ---- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ----
    config_dict = {}
    if args.config:
        try:
            config_dict = load_config_from_json(args.config)
            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {args.config}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            sys.exit(1)

    # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    cli_overrides = {}
    if args.headless is not None:
        cli_overrides['headless'] = args.headless
    if args.verbose is not None:
        cli_overrides['verbose'] = args.verbose
    if args.max_pages is not None:
        cli_overrides['max_pages_pdf'] = args.max_pages  # –¥–ª—è —ç—Ç–∞–ø–∞ PDF
    if args.workers is not None:
        cli_overrides['max_workers_pdf'] = args.workers

    merged_config = merge_config(config_dict, cli_overrides)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ç–∞–ø–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ PDF
    pdf_strict = not merged_config.get('loose', False)
    pdf_include_metadata = merged_config.get('include_metadata', False)
    pdf_max_pages = merged_config.get('max_pages_pdf', 10)
    pdf_max_workers = merged_config.get('max_workers_pdf', None)
    pdf_max_concurrent = merged_config.get('max_concurrent_pdf', None)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (ScraperConfig)
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å —Ç–æ–ª—å–∫–æ —Å —Ç–µ–º–∏ –∫–ª—é—á–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ ScraperConfig
    web_config_keys = [
        'headless', 'base_url', 'skip_main_page', 'use_fast_selectors',
        'delay_after_main', 'delay_after_search', 'delay_after_click',
        'delay_between_actions', 'wait_city_modal', 'wait_product_link',
        'poll_interval', 'no_product_phrases', 'max_tabs',
        'rate_limit_phrases', 'rate_limit_initial_delay',
        'rate_limit_coef_start', 'rate_limit_coef_step', 'rate_limit_coef_max',
        'handle_rate_limit', 'keep_browser_open', 'verbose',
        'api_max_concurrent'
    ]
    web_config_dict = {k: merged_config.get(k) for k in web_config_keys if k in merged_config}
    # –ï—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –∫–ª—é—á–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã, –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ ScraperConfig
    web_config = ScraperConfig(**web_config_dict)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if web_config.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        pdf_logger.setLevel(logging.DEBUG)
        # –û—Ç–∫–ª—é—á–∞–µ–º —à—É–º–Ω—ã–µ –ª–æ–≥–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        logging.getLogger("selenium").setLevel(logging.WARNING)
        logging.getLogger("websockets").setLevel(logging.WARNING)
    else:
        logging.getLogger().setLevel(logging.INFO)
        pdf_logger.setLevel(logging.INFO)

    # ---- –®–∞–≥ 1: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF ----
    logger.info("–≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF-—Ñ–∞–π–ª–æ–≤")
    pdf_results = await collect_isbns_from_pdfs(
        directory=args.directory,
        max_workers=pdf_max_workers,
        strict=pdf_strict,
        include_metadata=pdf_include_metadata,
        max_pages=pdf_max_pages,
        max_concurrent=pdf_max_concurrent,
    )

    if not pdf_results:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ PDF-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    # ---- –®–∞–≥ 2: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ISBN ----
    isbn_mapping, unique_isbns = build_isbn_mapping(pdf_results)
    if not unique_isbns:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ ISBN –≤ PDF-—Ñ–∞–π–ª–∞—Ö")
        print_pdf_results_table(pdf_results, {})
        return

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ISBN: {len(unique_isbns)}")
    logger.debug(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ISBN: {unique_isbns}")

    # ---- –®–∞–≥ 3: –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ API –∏ –†–ì–ë ----
    logger.info("–≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API –∏ –†–ì–ë")
    api_results, remaining_isbns, remaining_indices = await run_api_stage(unique_isbns, web_config)

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã API –≤ —Å–ª–æ–≤–∞—Ä—å
    book_data = {}
    for i, res in enumerate(api_results):
        if res:
            book_data[unique_isbns[i]] = res

    # ---- –®–∞–≥ 4: —Å–∫—Ä–∞–ø–∏–Ω–≥ –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è ISBN ----
    if remaining_isbns:
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å ISBN –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {len(remaining_isbns)}")
        scraped_results = await run_scraping_stage(remaining_isbns, remaining_indices, web_config)
        # –û–±–Ω–æ–≤–ª—è–µ–º book_data
        for local_idx, res in enumerate(scraped_results):
            if res:
                isbn = remaining_isbns[local_idx]
                book_data[isbn] = res
    else:
        logger.info("–í—Å–µ ISBN –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ API/–†–ì–ë, —Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

    # ---- –®–∞–≥ 5: –≤—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã ----
    print_pdf_results_table(pdf_results, book_data)

    # ---- –®–∞–≥ 6: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON ----
    if args.output:
        save_results_to_json(pdf_results, book_data, args.output)


def main():
    parser = argparse.ArgumentParser(
        description="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF –∏ –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö (API + —Å–∫—Ä–∞–ø–∏–Ω–≥)"
    )
    parser.add_argument("directory", help="–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ PDF")
    parser.add_argument("--headless", action="store_true",
                        help="–ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ")
    parser.add_argument("--config", type=str, default=None,
                        help="–ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—Å–º. –ø—Ä–∏–º–µ—Ä)")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--output", "-o", type=str, default=None,
                        help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Ñ–∞–π–ª")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
    parser.add_argument("--max-pages", type=int, default=None,
                        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PDF (0 = –≤—Å–µ)")
    parser.add_argument("--workers", type=int, default=None,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —á–∏—Å–ª–æ —è–¥–µ—Ä CPU)")
    parser.add_argument("--loose", action="store_true",
                        help="–ù–µ—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ ISBN –≤ PDF (–±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ ISBN)")
    parser.add_argument("--include-metadata", action="store_true",
                        help="–ü—Ä–æ–≤–µ—Ä—è—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ PDF –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É")

    args = parser.parse_args()

    asyncio.run(async_main(args))


if __name__ == "__main__":
    main()