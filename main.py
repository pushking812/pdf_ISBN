#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN –∏–∑ PDF-—Ñ–∞–π–ª–æ–≤ –∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- pdf_extract_isbn.py –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN –∏–∑ PDF
- web_scraper_isbn.py –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ ISBN (API –∏ —Å–∫—Ä–∞–ø–∏–Ω–≥)

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ JSON-—Ñ–∞–π–ª (–ø–∞—Ä–∞–º–µ—Ç—Ä --config) –∏–ª–∏ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
(—Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, headless, verbose, output). –î–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ JSON.
"""

import asyncio
import argparse
import json
import logging
import sys
import time
import random
from typing import Dict, List, Optional, Tuple, Any, Callable
from collections import defaultdict
from functools import partial
import os

# –ò–º–ø–æ—Ä—Ç –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from pdf_extract_isbn import (
    find_pdf_files,
    extract_isbn_from_pdf,
    logger as pdf_logger,
)
from config import ScraperConfig
from utils import normalize_isbn
from drivers import create_chrome_driver
from scraper import (
    run_api_stage,
    async_parallel_search,
    RussianBookScraperUC,
    TabState,
    TabInfo,
)
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("main")

# ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==========

def truncate_path(path: str, max_len: int = 60) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–µ—Ü, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π."""
    if len(path) <= max_len:
        return path
    return "..." + path[-(max_len - 3):]


def load_config_from_json(json_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON-—Ñ–∞–π–ª–∞."""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def merge_config(base_config: dict, cli_args: dict) -> dict:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã CLI –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç. –°–ª—É–∂–µ–±–Ω—ã–µ –∫–ª—é—á–∏ (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å _) –Ω–µ –∫–æ–ø–∏—Ä—É—é—Ç—Å—è.
    """
    merged = {k: v for k, v in base_config.items() if not (isinstance(k, str) and k.startswith('_'))}
    for key, value in cli_args.items():
        if value is not None:  # —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç —è–≤–Ω–æ –∑–∞–¥–∞–Ω
            merged[key] = value
    return merged


# ========== –ö–≠–®–ò –î–õ–Ø –£–°–ö–û–†–ï–ù–ò–Ø –ü–û–í–¢–û–†–ù–´–• –ó–ê–ü–£–°–ö–û–í ==========

CACHE_VERSION = 1

# –§–æ—Ä–º–∞—Ç PDF-–∫—ç—à–∞: –∫–ª—é—á = "–∏–º—è_—Ñ–∞–π–ª–∞|—Ä–∞–∑–º–µ—Ä" (–±–µ–∑ –ø—É—Ç–∏); —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ –∏ —Ä–∞–∑–º–µ—Ä—É = –æ–¥–∏–Ω —Ñ–∞–π–ª
# entries: { "filename.pdf|12345": { "isbn": str|None, "source": str, "mtime": int, "size": int } }
# –§–æ—Ä–º–∞—Ç –∫—ç—à–∞ –∫–Ω–∏–≥: { "version": 1, "entries": { "isbn13": { "title", "authors", "source", "pages", "year", ... } } }


def load_pdf_cache(path: str) -> Dict[str, Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à PDF‚ÜíISBN. –ö–ª—é—á–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–∏–º—è_—Ñ–∞–π–ª–∞|—Ä–∞–∑–º–µ—Ä¬ª. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–∏–≥—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç (–ø—É—Ç—å)."""
    if not path or not os.path.isfile(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if data.get('version') != CACHE_VERSION:
            return {}
        entries = data.get('entries', {})
        return _migrate_pdf_cache_to_name_size(entries)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å PDF-–∫—ç—à %s: %s", path, e)
        return {}


def save_pdf_cache(entries: Dict[str, Dict[str, Any]], path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à PDF‚ÜíISBN."""
    if not path:
        return
    try:
        os.makedirs(os.path.dirname(os.path.abspath(path)) or '.', exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump({"version": CACHE_VERSION, "entries": entries}, f, ensure_ascii=False, indent=2)
        logger.debug("PDF-–∫—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %s", path)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å PDF-–∫—ç—à %s: %s", path, e)


def load_isbn_cache(path: str) -> Dict[str, Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à ISBN‚Üí–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å entries –∏–ª–∏ –ø—É—Å—Ç–æ–π dict."""
    if not path or not os.path.isfile(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if data.get('version') != CACHE_VERSION:
            return {}
        return data.get('entries', {})
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –∫–Ω–∏–≥ %s: %s", path, e)
        return {}


def save_isbn_cache(entries: Dict[str, Dict[str, Any]], path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à ISBN‚Üí–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏."""
    if not path:
        return
    try:
        os.makedirs(os.path.dirname(os.path.abspath(path)) or '.', exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump({"version": CACHE_VERSION, "entries": entries}, f, ensure_ascii=False, indent=2)
        logger.debug("–ö—ç—à –∫–Ω–∏–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %s", path)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –∫–Ω–∏–≥ %s: %s", path, e)


def is_book_data_complete(record: Optional[Dict[str, Any]]) -> bool:
    """–°—á–∏—Ç–∞–µ–º –∑–∞–ø–∏—Å—å –ø–æ–ª–Ω–æ–π, –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–ø—É—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)."""
    return bool(record and record.get('title'))


def pdf_cache_key(pdf_path: str) -> Optional[str]:
    """
    –ö–ª—é—á –∫—ç—à–∞ PDF: —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Ä–∞–∑–º–µ—Ä ‚Äî ¬´–∏–º—è|—Ä–∞–∑–º–µ—Ä¬ª.
    –û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª –≤ —Ä–∞–∑–Ω—ã—Ö –ø–∞–ø–∫–∞—Ö –¥–∞—ë—Ç –æ–¥–∏–Ω –∫–ª—é—á; —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ –∏ —Ä–∞–∑–º–µ—Ä—É = –æ–¥–∏–Ω —Ñ–∞–π–ª.
    """
    try:
        st = os.stat(pdf_path)
        return f"{os.path.basename(pdf_path)}|{st.st_size}"
    except OSError:
        return None


def _migrate_pdf_cache_to_name_size(entries: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Å—Ç–∞—Ä—ã–π –∫—ç—à (–∫–ª—é—á = –ø—É—Ç—å) –≤ —Ñ–æ—Ä–º–∞—Ç –∫–ª—é—á = –∏–º—è_—Ñ–∞–π–ª–∞|—Ä–∞–∑–º–µ—Ä."""
    result = {}
    for key, value in entries.items():
        if "|" in key and key.count("|") == 1 and isinstance(value.get("size"), (int, float)):
            result[key] = value
            continue
        name = os.path.basename(key) if (os.path.sep in key or "/" in key) else key
        size = value.get("size")
        if size is not None:
            result[f"{name}|{size}"] = value
        else:
            result[key] = value
    return result


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –≠–¢–ê–ü–ê –°–ö–†–ê–ü–ò–ù–ì–ê –° –¢–ê–ë–õ–ò–ß–ù–´–ú –ü–†–û–ì–†–ï–°–°–û–ú ==========

def parallel_search_with_progress(
    isbn_list: List[str],
    config: ScraperConfig,
    progress_callback: Callable[[int, Optional[Dict[str, Any]]], None]
) -> List[Optional[Dict[str, Any]]]:
    """
    –ê–Ω–∞–ª–æ–≥ async_parallel_search –∏–∑ web_scraper_isbn, –Ω–æ —Å –≤—ã–∑–æ–≤–æ–º callback
    –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ ISBN (–ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏).
    progress_callback(index, result) –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –¥–ª—è ISBN —Å –¥–∞–Ω–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
    –ø–æ–ª—É—á–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω).
    """
    # –î—Ä–∞–π–≤–µ—Ä —Å —Ç–∞–π–º–∞—É—Ç–∞–º–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—É—Å–∫–æ—Ä—è–µ—Ç —Å–∫—Ä–∞–ø–∏–Ω–≥)
    driver = create_chrome_driver(config)
    delay_tab = getattr(config, 'delay_tab_switch', 0.2)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not config.skip_main_page:
        driver.get(config.base_url)
        time.sleep(random.uniform(*config.delay_after_main))
        try:
            WebDriverWait(driver, config.wait_city_modal).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), '–î–∞, —è –∑–¥–µ—Å—å')]"))
            ).click()
            if config.verbose:
                logger.debug("–ì–æ—Ä–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω (–≥–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ)")
            time.sleep(random.uniform(*config.delay_between_actions))
        except:
            pass
    else:
        if config.verbose:
            logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ ISBN –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ max_tabs
    chunks = [isbn_list[i:i + config.max_tabs] for i in range(0, len(isbn_list), config.max_tabs)]
    all_results = [None] * len(isbn_list)
    rate_limit_attempts = 0

    # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç scraper –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
    scraper_template = RussianBookScraperUC(config)

    for chunk_idx, chunk in enumerate(chunks):
        if config.verbose:
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {chunk_idx + 1}/{len(chunks)} (ISBN: {chunk})")

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
        handles = []
        try:
            main_handle = driver.current_window_handle
            handles.append(main_handle)
            for i in range(1, len(chunk)):
                driver.switch_to.new_window('tab')
                time.sleep(delay_tab)
                new_handle = driver.current_window_handle
                if new_handle not in handles:
                    handles.append(new_handle)
                else:
                    all_handles = driver.window_handles
                    found = False
                    for h in all_handles:
                        if h not in handles:
                            handles.append(h)
                            found = True
                            break
                    if not found:
                        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–π handle –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ {i}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_idx}: {e}")
            # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ ISBN —á–∞–Ω–∫–∞ –∫–∞–∫ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            for j, _ in enumerate(chunk):
                idx = chunk_idx * config.max_tabs + j
                all_results[idx] = None
                progress_callback(idx, None)
            time.sleep(1)
            continue

        # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç—ã TabInfo —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
        tabs = []
        for i, isbn in enumerate(chunk):
            global_idx = chunk_idx * config.max_tabs + i
            tabs.append(TabInfo(isbn, handles[i], global_idx, config))

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞–∂–¥—É—é –≤–∫–ª–∞–¥–∫—É
        for tab in tabs:
            try:
                driver.switch_to.window(tab.handle)
                clean_isbn = tab.isbn.replace("-", "").strip()
                search_url = f"{config.base_url}/search?phrase={clean_isbn}"
                driver.get(search_url)
                tab.state = TabState.SEARCHING
                tab.search_start_time = time.time()
                time.sleep(delay_tab)
            except Exception as e:
                logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–∏—Å–∫: {e}")
                tab.state = TabState.ERROR
                progress_callback(tab.index, None)

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞
        all_done = False
        while not all_done:
            all_done = True
            for tab in tabs:
                if tab.state in (TabState.DONE, TabState.ERROR, TabState.RATE_LIMITED):
                    continue
                all_done = False

                try:
                    driver.switch_to.window(tab.handle)
                except Exception as e:
                    logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è: {e}")
                    tab.state = TabState.ERROR
                    progress_callback(tab.index, None)
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if config.handle_rate_limit:
                    try:
                        page_source = driver.page_source
                        found_rate_limit = any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases)
                        if found_rate_limit:
                            logger.warning(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                            rate_limit_attempts += 1
                            coef = config.rate_limit_coef_start + (rate_limit_attempts - 1) * config.rate_limit_coef_step
                            coef = min(coef, config.rate_limit_coef_max)
                            wait_time = config.rate_limit_initial_delay * coef
                            logger.info(f"–ü–∞—É–∑–∞ {wait_time:.1f}—Å (–∫–æ—ç—Ñ. {coef:.2f})")
                            time.sleep(wait_time)
                            driver.refresh()
                            while True:
                                time.sleep(config.poll_interval)
                                page_source = driver.page_source
                                if any(phrase.lower() in page_source.lower() for phrase in config.rate_limit_phrases):
                                    time.sleep(wait_time)
                                    driver.refresh()
                                else:
                                    logger.debug("–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–Ω—è—Ç–∞, —Å–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞")
                                    rate_limit_attempts = 0
                                    break
                            for t in tabs:
                                if t.handle != tab.handle:
                                    try:
                                        driver.switch_to.window(t.handle)
                                        driver.refresh()
                                        time.sleep(0.5)
                                    except:
                                        pass
                            driver.switch_to.window(tab.handle)
                            break
                        else:
                            if rate_limit_attempts > 0:
                                rate_limit_attempts = 0
                    except Exception:
                        pass

                if tab.state == TabState.SEARCHING:
                    try:
                        page_source = driver.page_source
                        if any(phrase in page_source for phrase in config.no_product_phrases):
                            logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
                            tab.state = TabState.ERROR
                            progress_callback(tab.index, None)
                            continue
                    except:
                        pass

                    elapsed = time.time() - tab.search_start_time
                    found_link = None
                    for selector in scraper_template.product_link_selectors:
                        try:
                            element = driver.find_element(By.CSS_SELECTOR, selector)
                            found_link = element
                            break
                        except NoSuchElementException:
                            continue
                    if found_link:
                        tab.book_url = found_link.get_attribute('href')
                        driver.get(tab.book_url)
                        time.sleep(random.uniform(*config.delay_after_click))
                        tab.state = TabState.BOOK_PAGE
                        logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏")
                    else:
                        if elapsed > tab.timeout:
                            logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç")
                            tab.state = TabState.ERROR
                            progress_callback(tab.index, None)

                elif tab.state == TabState.BOOK_PAGE:
                    try:
                        scraper_template.driver = driver
                        result = scraper_template._parse_book_page()
                        tab.result = result
                        tab.state = TabState.DONE
                        rate_limit_attempts = 0
                        all_results[tab.index] = result
                        progress_callback(tab.index, result)
                        logger.debug(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] ISBN {tab.isbn} –≥–æ—Ç–æ–≤")
                    except Exception as e:
                        logger.error(f"[–í–∫–ª–∞–¥–∫–∞ {tab.index}] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                        tab.state = TabState.ERROR
                        progress_callback(tab.index, None)

            if not all_done:
                time.sleep(config.poll_interval)

        # –ó–∞–∫—Ä—ã—Ç–∏–µ –≤–∫–ª–∞–¥–æ–∫ —á–∞–Ω–∫–∞ (–∫—Ä–æ–º–µ –≥–ª–∞–≤–Ω–æ–π)
        main_handle = handles[0]
        for handle in handles[1:]:
            try:
                driver.switch_to.window(handle)
                driver.close()
            except:
                pass
        driver.switch_to.window(main_handle)
        time.sleep(1)

    if not config.keep_browser_open:
        driver.quit()
    else:
        input("\nüîç –ë—Ä–∞—É–∑–µ—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–º. –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è...")
        driver.quit()

    return all_results


async def run_scraping_stage(
    isbn_list: List[str],
    indices: List[int],  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    config: ScraperConfig
) -> List[Optional[Dict[str, Any]]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∫—Ä–∞–ø–∏–Ω–≥ –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥–∞ –¥–ª—è —Å–ø–∏—Å–∫–∞ ISBN —Å –≤—ã–≤–æ–¥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ç–∞–±–ª–∏—Ü—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä —Ä–∞–≤–µ–Ω len(isbn_list)) –≤ –ø–æ—Ä—è–¥–∫–µ isbn_list.
    """
    if not isbn_list:
        return []

    print(f"\nüîç –°–∫—Ä–∞–ø–∏–Ω–≥ –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥ –¥–ª—è {len(isbn_list)} ISBN (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ):")
    header = f"{' ‚Ññ':>4} | {'ISBN':<20} | {'–°—Ç–∞—Ç—É—Å':<25} | {'–ù–∞–∑–≤–∞–Ω–∏–µ'}"
    print(header)
    print("-" * len(header))

    results = [None] * len(isbn_list)

    def progress_callback(idx: int, res: Optional[Dict[str, Any]]):
        # idx - –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ isbn_list (0..len(isbn_list)-1)
        isbn = isbn_list[idx]
        if res:
            title = res.get('title', '')[:47] + '...' if len(res.get('title', '')) > 50 else res.get('title', '')
            status = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ ({res.get('source', '–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥')})"
            print(f"{idx+1:4} | {isbn:<20} | {status:<25} | {title}")
        else:
            print(f"{idx+1:4} | {isbn:<20} | {'‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ':<25} |")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é parallel_search_with_progress –≤ executor
    loop = asyncio.get_running_loop()
    scraped_results = await loop.run_in_executor(
        None,
        parallel_search_with_progress,
        isbn_list,
        config,
        progress_callback
    )
    print("-" * len(header))
    return scraped_results


# ========== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ==========

async def collect_isbns_from_pdfs(
    directory: str,
    max_workers: Optional[int] = None,
    strict: bool = True,
    include_metadata: bool = False,
    max_pages: int = 10,
    max_concurrent: Optional[int] = None,
    use_pdf_cache: bool = True,
    pdf_cache: Optional[Dict[str, Dict[str, Any]]] = None,
    pdf_cache_path: Optional[str] = None,
    rescan: bool = False,
) -> Tuple[List[Tuple[str, Optional[str], str]], Dict[str, Dict[str, Any]]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –∏–∑–≤–ª–µ–∫–∞–µ—Ç ISBN –∏–∑ PDF (—Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫—ç—à–µ–º).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π pdf_cache –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è).
    """
    from concurrent.futures import ProcessPoolExecutor

    pdf_cache = pdf_cache if pdf_cache is not None else {}
    pdf_files = await find_pdf_files(directory)
    if not pdf_files:
        logger.info("PDF-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ %s", directory)
        return [], pdf_cache

    use_cache = use_pdf_cache and not rescan
    extract_func = partial(
        extract_isbn_from_pdf,
        strict=strict,
        include_metadata=include_metadata,
        max_pages=max_pages,
    )

    cached_results: Dict[str, Tuple[Optional[str], str]] = {}
    uncached_paths: List[str] = []
    for path in pdf_files:
        cache_key = pdf_cache_key(path)
        if use_cache and cache_key and cache_key in pdf_cache:
            entry = pdf_cache[cache_key]
            cached_results[path] = (entry.get('isbn'), entry.get('source', 'text'))
        else:
            uncached_paths.append(path)

    if cached_results:
        logger.info("–ò–∑ PDF-–∫—ç—à–∞: %d —Ñ–∞–π–ª(–æ–≤)", len(cached_results))

    uncached_results: List[Tuple[str, Optional[str], str]] = []
    if uncached_paths:
        loop = asyncio.get_running_loop()
        sem_limit = max_concurrent or max_workers or (os.cpu_count() or 4)
        semaphore = asyncio.Semaphore(sem_limit)
        executor = ProcessPoolExecutor(max_workers=max_workers)

        async def extract_one(p: str) -> Tuple[str, Optional[str], str]:
            async with semaphore:
                isbn, source = await loop.run_in_executor(executor, extract_func, p)
            return p, isbn, source

        try:
            uncached_results = await asyncio.gather(*[extract_one(p) for p in uncached_paths])
            for path, isbn, source in uncached_results:
                ckey = pdf_cache_key(path)
                if ckey:
                    try:
                        st = os.stat(path)
                        pdf_cache[ckey] = {
                            "isbn": isbn,
                            "source": source,
                            "mtime": st.st_mtime,
                            "size": st.st_size,
                        }
                    except OSError:
                        pdf_cache[ckey] = {"isbn": isbn, "source": source}
                if isbn:
                    logger.debug("[%s] -> ISBN: %s (%s)", path, isbn, source)
                else:
                    logger.debug("[%s] -> ISBN –Ω–µ –Ω–∞–π–¥–µ–Ω", path)
        finally:
            executor.shutdown(wait=True)

    # –ò—Ç–æ–≥ –≤ –ø–æ—Ä—è–¥–∫–µ pdf_files
    result_list: List[Tuple[str, Optional[str], str]] = []
    uncached_by_path = {t[0]: (t[1], t[2]) for t in uncached_results}
    for path in pdf_files:
        if path in cached_results:
            isbn, source = cached_results[path]
            result_list.append((path, isbn, source))
        else:
            isbn, source = uncached_by_path[path]
            result_list.append((path, isbn, source))

    logger.info("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ PDF: %d (–∏–∑ –∫—ç—à–∞: %d, –∏–∑–≤–ª–µ—á–µ–Ω–æ: %d)", len(result_list), len(cached_results), len(uncached_paths))
    return result_list, pdf_cache


def build_isbn_mapping(
    pdf_results: List[Tuple[str, Optional[str], str]]
) -> Tuple[Dict[str, List[Tuple[str, str, str]]], List[str]]:
    """
    –°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å: isbn -> —Å–ø–∏—Å–æ–∫ (–ø—É—Ç—å_–∫_pdf, –∏—Å—Ç–æ—á–Ω–∏–∫, isbn_raw).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö ISBN.
    """
    mapping = defaultdict(list)
    unique_isbns = []
    for pdf_path, isbn, source in pdf_results:
        if isbn:
            norm_isbn = normalize_isbn(isbn)
            if norm_isbn:
                mapping[norm_isbn].append((pdf_path, source, isbn))
                if norm_isbn not in unique_isbns:
                    unique_isbns.append(norm_isbn)
            else:
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ISBN {isbn} –≤ —Ñ–∞–π–ª–µ {pdf_path} –ø—Ä–æ–ø—É—â–µ–Ω")
    return mapping, unique_isbns


def print_pdf_results_table(
    pdf_results: List[Tuple[str, Optional[str], str]],
    book_data: Dict[str, Optional[Dict[str, Any]]]
) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ PDF."""
    print("\n" + "=" * 140)
    header = (
        f"{'PDF —Ñ–∞–π–ª':<60} {'ISBN':<20} {'–ò—Å—Ç–æ—á–Ω–∏–∫':<10} "
        f"{'–ù–∞–∑–≤–∞–Ω–∏–µ':<40} {'–ê–≤—Ç–æ—Ä(—ã)':<30} {'–°—Ç—Ä.':<6} {'–ì–æ–¥':<5}"
    )
    print(header)
    print("=" * 140)

    for pdf_path, isbn, src in pdf_results:
        display_path = truncate_path(pdf_path, 60)

        if not isbn:
            print(f"{display_path:<60} {'‚Äî':<20} {'‚Äî':<10} {'‚ùå ISBN –Ω–µ –Ω–∞–π–¥–µ–Ω':<40} {'':<30} {'':<6} {'':<5}")
            continue

        norm_isbn = normalize_isbn(isbn)
        if not norm_isbn:
            print(f"{display_path:<60} {isbn:<20} {src:<10} {'‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ISBN':<40} {'':<30} {'':<6} {'':<5}")
            continue

        data = book_data.get(norm_isbn)
        if not data:
            print(f"{display_path:<60} {isbn:<20} {src:<10} {'‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞':<40} {'':<30} {'':<6} {'':<5}")
            continue

        # –£—Å–µ–∫–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        title = data.get('title', '')[:37] + '...' if len(data.get('title', '')) > 40 else data.get('title', '')
        authors = ', '.join(data.get('authors', []))[:27] + '...' if len(', '.join(data.get('authors', []))) > 30 else ', '.join(data.get('authors', []))
        pages = data.get('pages', '‚Äî')
        year = data.get('year', '‚Äî')
        source_web = data.get('source', '‚Äî')

        print(
            f"{display_path:<60} {isbn:<20} {src:<10} "
            f"{title:<40} {authors:<30} {pages:<6} {year:<5} "
        )
    print("=" * 140)


def save_results_to_json(
    pdf_results: List[Tuple[str, Optional[str], str]],
    book_data: Dict[str, Optional[Dict[str, Any]]],
    output_file: str
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Ñ–∞–π–ª."""
    output = []
    for pdf_path, isbn, src in pdf_results:
        record = {
            "pdf_path": pdf_path,
            "isbn_raw": isbn,
            "source_pdf": src,
        }
        if isbn:
            norm_isbn = normalize_isbn(isbn)
            if norm_isbn and norm_isbn in book_data:
                record["book_info"] = book_data[norm_isbn]
            else:
                record["book_info"] = None
        else:
            record["book_info"] = None
        output.append(record)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


def load_book_data_from_results_json(path: str) -> Dict[str, Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–Ω–∏–≥–∞—Ö –∏–∑ JSON-–æ—Ç—á—ë—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç --output).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å isbn -> book_info –¥–ª—è –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–ø—É—Å—Ç—ã–º book_info.
    """
    if not path or not os.path.isfile(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å JSON –æ—Ç—á—ë—Ç–∞ %s: %s", path, e)
        return {}
    result = {}
    for record in data if isinstance(data, list) else []:
        isbn_raw = record.get('isbn_raw')
        book_info = record.get('book_info')
        if not isbn_raw or not book_info or not isinstance(book_info, dict):
            continue
        norm = normalize_isbn(isbn_raw)
        if norm and is_book_data_complete(book_info):
            result[norm] = book_info
    return result


async def async_main(args):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""

    # ---- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ----
    config_path = args.config
    if config_path is None and os.path.isfile("config.json"):
        config_path = "config.json"
    config_dict = {}
    if config_path:
        try:
            config_dict = load_config_from_json(config_path)
            logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ %s", config_path)
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: %s", e)
            sys.exit(1)

    # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    cli_overrides = {}
    if args.headless is not None:
        cli_overrides['headless'] = args.headless
    if args.verbose is not None:
        cli_overrides['verbose'] = args.verbose
    if args.max_pages is not None:
        cli_overrides['max_pages_pdf'] = args.max_pages  # –¥–ª—è —ç—Ç–∞–ø–∞ PDF
    if args.workers is not None:
        cli_overrides['max_workers_pdf'] = args.workers

    merged_config = merge_config(config_dict, cli_overrides)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ç–∞–ø–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ PDF
    pdf_strict = not merged_config.get('loose', False)
    pdf_include_metadata = merged_config.get('include_metadata', False)
    pdf_max_pages = merged_config.get('max_pages_pdf', 10)
    pdf_max_workers = merged_config.get('max_workers_pdf', None)
    pdf_max_concurrent = merged_config.get('max_concurrent_pdf', None)
    pdf_isbn_cache_path = merged_config.get('pdf_isbn_cache', 'pdf_isbn_cache.json')
    isbn_data_cache_path = merged_config.get('isbn_data_cache', 'isbn_data_cache.json')
    rescan = getattr(args, 'rescan', False)
    use_pdf_cache = not rescan
    use_isbn_cache = not rescan

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (ScraperConfig)
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å —Ç–æ–ª—å–∫–æ —Å —Ç–µ–º–∏ –∫–ª—é—á–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ ScraperConfig
    web_config_keys = [
        'headless', 'base_url', 'skip_main_page', 'use_fast_selectors',
        'delay_after_main', 'delay_after_search', 'delay_after_click',
        'delay_between_actions', 'wait_city_modal', 'wait_product_link',
        'poll_interval', 'no_product_phrases', 'max_tabs',
        'rate_limit_phrases', 'rate_limit_initial_delay',
        'rate_limit_coef_start', 'rate_limit_coef_step', 'rate_limit_coef_max',
        'handle_rate_limit', 'keep_browser_open', 'verbose',
        'api_max_concurrent',
        'page_load_timeout', 'page_load_strategy', 'delay_tab_switch'
    ]
    web_config_dict = {k: merged_config.get(k) for k in web_config_keys if k in merged_config}
    # –ï—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –∫–ª—é—á–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã, –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ ScraperConfig
    web_config = ScraperConfig(**web_config_dict)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if web_config.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        pdf_logger.setLevel(logging.DEBUG)
        # –û—Ç–∫–ª—é—á–∞–µ–º —à—É–º–Ω—ã–µ –ª–æ–≥–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        logging.getLogger("selenium").setLevel(logging.WARNING)
        logging.getLogger("websockets").setLevel(logging.WARNING)
    else:
        logging.getLogger().setLevel(logging.INFO)
        pdf_logger.setLevel(logging.INFO)

    # ---- –ö—ç—à–∏ ----
    pdf_cache = load_pdf_cache(pdf_isbn_cache_path) if use_pdf_cache else {}
    isbn_cache = load_isbn_cache(isbn_data_cache_path) if use_isbn_cache else {}
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON-–æ—Ç—á—ë—Ç–∞ (--output), –µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å
    if use_isbn_cache and getattr(args, 'output', None) and os.path.isfile(args.output):
        from_output = load_book_data_from_results_json(args.output)
        if from_output:
            isbn_cache.update(from_output)
            logger.info("–î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ JSON-–æ—Ç—á—ë—Ç–∞ %s: %d –∑–∞–ø–∏—Å–µ–π", args.output, len(from_output))
    if use_pdf_cache and pdf_cache:
        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω PDF-–∫—ç—à: %s (%d –∑–∞–ø–∏—Å–µ–π)", pdf_isbn_cache_path, len(pdf_cache))
    if use_isbn_cache and isbn_cache:
        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à –∫–Ω–∏–≥: %s (%d –∑–∞–ø–∏—Å–µ–π)", isbn_data_cache_path, len(isbn_cache))

    # ---- –®–∞–≥ 1: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF ----
    logger.info("–≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF-—Ñ–∞–π–ª–æ–≤")
    pdf_results, pdf_cache = await collect_isbns_from_pdfs(
        directory=args.directory,
        max_workers=pdf_max_workers,
        strict=pdf_strict,
        include_metadata=pdf_include_metadata,
        max_pages=pdf_max_pages,
        max_concurrent=pdf_max_concurrent,
        use_pdf_cache=use_pdf_cache,
        pdf_cache=pdf_cache,
        pdf_cache_path=pdf_isbn_cache_path,
        rescan=rescan,
    )
    if use_pdf_cache and pdf_isbn_cache_path:
        save_pdf_cache(pdf_cache, pdf_isbn_cache_path)

    if not pdf_results:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ PDF-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    # ---- –®–∞–≥ 2: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ISBN ----
    isbn_mapping, unique_isbns = build_isbn_mapping(pdf_results)
    if not unique_isbns:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ ISBN –≤ PDF-—Ñ–∞–π–ª–∞—Ö")
        print_pdf_results_table(pdf_results, {})
        return

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ISBN: {len(unique_isbns)}")
    logger.debug(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ISBN: {unique_isbns}")

    # ---- –®–∞–≥ 3: –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–Ω–∏–≥–∞–º (–∫—ç—à + API + —Å–∫—Ä–∞–ø–∏–Ω–≥) ----
    book_data: Dict[str, Optional[Dict[str, Any]]] = {}
    for isbn in unique_isbns:
        if use_isbn_cache and isbn in isbn_cache and is_book_data_complete(isbn_cache[isbn]):
            book_data[isbn] = isbn_cache[isbn]
    remaining_to_fetch = [isbn for isbn in unique_isbns if isbn not in book_data]
    if not remaining_to_fetch:
        logger.info("–í—Å–µ ISBN –Ω–∞–π–¥–µ–Ω—ã –≤ –∫—ç—à–µ –∫–Ω–∏–≥, –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è.")
    else:
        if use_isbn_cache and book_data:
            logger.info("–ò–∑ –∫—ç—à–∞ –∫–Ω–∏–≥: %d, –∑–∞–ø—Ä–æ—Å –¥–ª—è: %d", len(book_data), len(remaining_to_fetch))
        logger.info("–≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API –∏ –†–ì–ë")
        api_results, remaining_isbns, remaining_indices = await run_api_stage(remaining_to_fetch, web_config)
        for i, res in enumerate(api_results):
            if res:
                book_data[remaining_to_fetch[i]] = res

        if remaining_isbns:
            logger.info("–û—Å—Ç–∞–ª–æ—Å—å ISBN –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: %d", len(remaining_isbns))
            scraped_results = await run_scraping_stage(remaining_isbns, remaining_indices, web_config)
            for local_idx, res in enumerate(scraped_results):
                if res:
                    book_data[remaining_isbns[local_idx]] = res
        else:
            logger.info("–í—Å–µ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ ISBN –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ API/–†–ì–ë, —Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∫–Ω–∏–≥ –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    if use_isbn_cache and isbn_data_cache_path:
        isbn_cache.update({k: v for k, v in book_data.items() if v and is_book_data_complete(v)})
        save_isbn_cache(isbn_cache, isbn_data_cache_path)

    # ---- –®–∞–≥ 5: –≤—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã ----
    print_pdf_results_table(pdf_results, book_data)

    # ---- –®–∞–≥ 6: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON ----
    if args.output:
        save_results_to_json(pdf_results, book_data, args.output)


def main():
    parser = argparse.ArgumentParser(
        description="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ISBN –∏–∑ PDF –∏ –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö (API + —Å–∫—Ä–∞–ø–∏–Ω–≥)"
    )
    parser.add_argument("directory", help="–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ PDF")
    parser.add_argument("--headless", action="store_true",
                        help="–ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ")
    parser.add_argument("--config", type=str, default=None,
                        help="–ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—Å–º. –ø—Ä–∏–º–µ—Ä)")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--output", "-o", type=str, default=None,
                        help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Ñ–∞–π–ª")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
    parser.add_argument("--max-pages", type=int, default=None,
                        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PDF (0 = –≤—Å–µ)")
    parser.add_argument("--workers", type=int, default=None,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ISBN (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —á–∏—Å–ª–æ —è–¥–µ—Ä CPU)")
    parser.add_argument("--loose", action="store_true",
                        help="–ù–µ—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ ISBN –≤ PDF (–±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ ISBN)")
    parser.add_argument("--include-metadata", action="store_true",
                        help="–ü—Ä–æ–≤–µ—Ä—è—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ PDF –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É")
    parser.add_argument("--rescan", action="store_true",
                        help="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à–∏ PDF –∏ –∫–Ω–∏–≥, –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –∑–∞–Ω–æ–≤–æ")

    args = parser.parse_args()

    asyncio.run(async_main(args))


if __name__ == "__main__":
    main()