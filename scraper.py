"""
–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å —Å–∫—Ä–∞–ø–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∑–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç:
1. –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∏–∑ scraper_core.orchestrator.core
2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∏–∑ scraper_core.config
3. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å debug_selectors —á–µ—Ä–µ–∑ scraper_core.parsers
4. –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º —á–µ—Ä–µ–∑ scraper_core.orchestrator.legacy_adapter
"""

import asyncio
import time
import random
from typing import Any, Dict, List, Optional, Tuple

from bs4 import BeautifulSoup

# –ò–º–ø–æ—Ä—Ç –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
from scraper_core.orchestrator.legacy_adapter import (
    TabState,
    TabInfo,
    async_parallel_search as new_async_parallel_search,
    process_isbn_async as new_process_isbn_async,
    run_api_stage as new_run_api_stage,
    search_multiple_books as new_search_multiple_books,
)
from scraper_core.integration.selector_integration import SelectorIntegration

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
from config import ScraperConfig


def parse_book_page_for_resource(
    driver: Any, resource: Dict[str, Any]
) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–∞.

    –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É—Å—Ç–∞—Ä–µ–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∏–∑ scraper_core.parsers.selector_client.

    Args:
        driver: WebDriver Selenium
        resource: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–∞

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–Ω–∏–≥–∏
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —á–µ—Ä–µ–∑ SelectorClient
    from scraper_core.parsers.selector_client import SelectorClient

    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
    selector_client = SelectorClient({})

    # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    html = driver.page_source

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –Ω–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
    result = selector_client.extract_with_selectors(
        html=html,
        selectors=resource.get("selectors", []),
        resource_id=resource.get("id", "unknown"),
    )

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
    if result:
        return {
            "title": result.get("title", "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ"),
            "authors": result.get("authors", ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"]),
            "pages": result.get("pages", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"),
            "year": result.get("year", "–Ω–µ —É–∫–∞–∑–∞–Ω"),
            "url": driver.current_url,
            "source": resource.get("source_label", "–°–∞–π—Ç"),
            "isbn": result.get("isbn", ""),
            "confidence": result.get("confidence", 0.0),
        }

    # –ï—Å–ª–∏ –Ω–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    custom_parser = resource.get("custom_parser")
    if custom_parser is not None:
        return custom_parser(driver, resource)

    soup = BeautifulSoup(driver.page_source, "lxml")
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É "–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    no_product_phrases = resource.get("no_product_phrases", [])
    page_text = soup.get_text().lower()
    if any(phrase.lower() in page_text for phrase in no_product_phrases if phrase):
        return None

    title = None
    for sel in resource.get("title_selectors", []):
        elem = soup.select_one(sel)
        if elem:
            if getattr(elem, "name", "") == "meta":
                title = elem.get("content", "").strip()
            else:
                title = elem.get_text(strip=True)
            break

    if resource.get("id") == "book-ru" and title:
        title = title.split(" - ISBN")[0].strip()

    authors = []
    for sel in resource.get("author_selectors", []):
        elems = soup.select(sel)
        if elems:
            authors = [a.get_text(strip=True) for a in elems if a.get_text(strip=True)]
            if resource.get("id") == "book-ru" and authors:
                authors = [authors[0].split(",")[0].strip()]
            break

    pages = None
    for sel in resource.get("pages_selectors", []):
        elem = soup.select_one(sel)
        if elem:
            pages = elem.get_text(strip=True)
            break

    year = None
    for sel in resource.get("year_selectors", []):
        elem = soup.select_one(sel)
        if elem:
            year = elem.get_text(strip=True)
            break

    if resource.get("properties_item_class"):
        for li in soup.find_all("li", class_=resource["properties_item_class"]):
            title_elem = li.find(
                "span", class_=resource.get("properties_title_class", "")
            )
            content_elem = li.find(
                "span", class_=resource.get("properties_content_class", "")
            )
            if title_elem and content_elem:
                text = title_elem.get_text(strip=True)
                lower_text = text.lower()
                if not pages and (
                    "—Å—Ç—Ä–∞–Ω–∏—Ü" in lower_text
                    or "—Å—Ç—Ä." in lower_text
                    or "–æ–±—ä–µ–º" in lower_text
                ):
                    pages = content_elem.get_text(strip=True)
                if not year and "–≥–æ–¥" in lower_text:
                    year_span = content_elem.find("span", itemprop="copyrightYear")
                    if year_span and year_span.get_text(strip=True):
                        year = year_span.get_text(strip=True)
                    else:
                        year = content_elem.get_text(strip=True)

    if resource.get("id") == "book-ru" and pages:
        import re

        m = re.search(r"\d+", pages)
        if m:
            pages = m.group()

    return {
        "title": title or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ",
        "authors": authors or ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"],
        "pages": pages or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
        "year": year or "–Ω–µ —É–∫–∞–∑–∞–Ω",
        "url": driver.current_url,
        "source": resource.get("source_label", "–°–∞–π—Ç"),
    }


class RussianBookScraperUC:
    """
    –°–∫—Ä–∞–ø–µ—Ä –¥–ª—è –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Undetected ChromeDriver.

    –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —É—Å—Ç–∞—Ä–µ–ª –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ResourceHandler –∏–∑ scraper_core.handlers.
    """

    def __init__(self, config: Any):
        self.config = config
        self.driver = None
        self._init_selectors()

    def _init_selectors(self):
        if self.config.use_fast_selectors:
            self.product_link_selectors = ['a[href^="/product/"]']
            self.title_selectors = ["h1.product-detail-page__title", "h1.product-title"]
            self.author_selectors = [".product-authors a"]
            self.pages_selectors = ['span[itemprop="numberOfPages"] span']
            self.year_selectors = ['span[itemprop="datePublished"] span']
        else:
            self.product_link_selectors = [
                'a[href^="/product/"]',
                "a.product-card__title",
                "a.product-title",
                ".catalog-item a",
            ]
            self.title_selectors = [
                "h1.product-detail-page__title",
                "h1.product-title",
                'h1[itemprop="name"]',
                ".product__title h1",
                "h1",
            ]
            self.author_selectors = [
                ".product-authors a",
                ".product-author a",
                'a[itemprop="author"]',
                ".product-info__author",
                ".authors-list a",
            ]
            self.pages_selectors = [
                'span[itemprop="numberOfPages"] span',
                '.product-properties-item span[itemprop="numberOfPages"]',
            ]
            self.year_selectors = [
                'span[itemprop="datePublished"] span',
                '.product-properties-item span[itemprop="datePublished"]',
            ]
        self.properties_item_class = "product-properties-item"
        self.properties_title_class = "product-properties-item__title"
        self.properties_content_class = "product-properties-item__content"

    def __enter__(self):
        from drivers import create_chrome_driver

        self.driver = create_chrome_driver(self.config)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver and not self.config.keep_browser_open:
            self.driver.quit()

    def _random_delay(self, delay_range: Tuple[float, float], msg: str = ""):
        delay = random.uniform(*delay_range)
        if msg and self.config.verbose:
            print(f"‚è±Ô∏è {msg}: {delay:.2f}—Å")
        time.sleep(delay)

    def _handle_city_modal(self):
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC

        try:
            city_button = WebDriverWait(self.driver, self.config.wait_city_modal).until(
                EC.element_to_be_clickable(
                    (By.XPATH, "//button[contains(text(), '–î–∞, —è –∑–¥–µ—Å—å')]")
                )
            )
            city_button.click()
            if self.config.verbose:
                print("üèôÔ∏è –ì–æ—Ä–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω")
            self._random_delay(self.config.delay_between_actions, "–ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞")
            return True
        except Exception:
            return False

    def _parse_book_page(self) -> Dict[str, Any]:
        soup = BeautifulSoup(self.driver.page_source, "lxml")
        title = None
        for sel in self.title_selectors:
            elem = soup.select_one(sel)
            if elem:
                title = elem.text.strip()
                break

        authors = []
        for sel in self.author_selectors:
            elems = soup.select(sel)
            if elems:
                authors = [a.text.strip() for a in elems if a.text.strip()]
                break

        pages = None
        for sel in self.pages_selectors:
            elem = soup.select_one(sel)
            if elem:
                pages = elem.text.strip()
                break

        year = None
        for sel in self.year_selectors:
            elem = soup.select_one(sel)
            if elem:
                year = elem.text.strip()
                break

        if not pages or not year:
            props = soup.find_all("li", class_=self.properties_item_class)
            for li in props:
                title_elem = li.find("span", class_=self.properties_title_class)
                if not title_elem:
                    continue
                text = title_elem.text.strip()
                content_elem = li.find("span", class_=self.properties_content_class)
                if not content_elem:
                    continue
                if "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü" in text and not pages:
                    pages = content_elem.text.strip()
                elif "–ì–æ–¥ –∏–∑–¥–∞–Ω–∏—è" in text and not year:
                    year = content_elem.text.strip()

        return {
            "title": title or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ",
            "authors": authors or ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"],
            "pages": pages or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
            "year": year or "–Ω–µ —É–∫–∞–∑–∞–Ω",
            "url": self.driver.current_url,
            "source": "–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥",
        }

    def search_by_isbn(self, isbn: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É ISBN (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –¥–ª—è –æ–¥–Ω–æ–π –≤–∫–ª–∞–¥–∫–∏)."""
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.common.exceptions import TimeoutException

        clean_isbn = isbn.replace("-", "").strip()
        search_url = f"{self.config.base_url}/search?phrase={clean_isbn}"
        try:
            if not self.config.skip_main_page:
                self.driver.get(self.config.base_url)
                self._random_delay(self.config.delay_after_main, "–ø–æ—Å–ª–µ –≥–ª–∞–≤–Ω–æ–π")
                self._handle_city_modal()
            else:
                if self.config.verbose:
                    print("‚è© –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (skip_main_page=True)")

            self.driver.get(search_url)
            self._random_delay(self.config.delay_after_search, "–ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞")
            self._handle_city_modal()

            product_link = None
            for selector in self.product_link_selectors:
                try:
                    product_link = WebDriverWait(
                        self.driver, self.config.wait_product_link
                    ).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    break
                except TimeoutException:
                    continue

            if not product_link:
                if self.config.verbose:
                    print("‚ùå –°—Å—ã–ª–∫–∞ –Ω–∞ –∫–Ω–∏–≥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
                return None

            book_url = product_link.get_attribute("href")
            self.driver.get(book_url)
            self._random_delay(self.config.delay_after_click, "–ø–æ—Å–ª–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ –∫–Ω–∏–≥—É")
            return self._parse_book_page()
        except Exception:
            return None


# –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
async def async_parallel_search(
    isbn_list: List[str], config: Optional[ScraperConfig] = None
) -> List[Optional[Dict[str, Any]]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É ISBN.

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.

    Args:
        isbn_list: –°–ø–∏—Å–æ–∫ ISBN –¥–ª—è –ø–æ–∏—Å–∫–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ISBN
    """
    return await new_async_parallel_search(isbn_list, config)


async def process_isbn_async(
    raw_isbn: str,
    config: Optional[ScraperConfig] = None,
    semaphore: Optional[asyncio.Semaphore] = None,
) -> Optional[Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ ISBN (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ).

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.

    Args:
        raw_isbn: ISBN –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        semaphore: –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –∏–ª–∏ None
    """
    return await new_process_isbn_async(raw_isbn, config, semaphore)


async def run_api_stage(
    isbn_list: List[str],
    config: Optional[ScraperConfig] = None,
    connector: Optional[Any] = None,
) -> List[Optional[Dict[str, Any]]]:
    """
    –ó–∞–ø—É—Å–∫ API-—Å—Ç–∞–¥–∏–∏ (Google Books, Open Library).

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.

    Args:
        isbn_list: –°–ø–∏—Å–æ–∫ ISBN –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ API
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        connector: –ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä aiohttp (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ API
    """
    return await new_run_api_stage(isbn_list, config, connector)


def search_multiple_books(
    isbn_list: List[str], config: Optional[ScraperConfig] = None
) -> List[Optional[Dict[str, Any]]]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º ISBN.

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.

    Args:
        isbn_list: –°–ø–∏—Å–æ–∫ ISBN –¥–ª—è –ø–æ–∏—Å–∫–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    return new_search_multiple_books(isbn_list, config)


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
def migrate_to_new_architecture(config_dir: str = "config") -> Dict[str, int]:
    """
    –ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.

    Args:
        config_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–∏–≥—Ä–∞—Ü–∏–∏
    """
    selector_integration = SelectorIntegration(config_dir)
    return selector_integration.migrate_existing_selectors()


def update_selectors_from_results(config_dir: str = "config") -> Dict[str, List[Dict]]:
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞.

    Args:
        config_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º
    """
    selector_integration = SelectorIntegration(config_dir)
    return selector_integration.auto_generate_all_selectors()


# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
__all__ = [
    "parse_book_page_for_resource",
    "TabState",
    "TabInfo",
    "RussianBookScraperUC",
    "async_parallel_search",
    "process_isbn_async",
    "run_api_stage",
    "search_multiple_books",
    "migrate_to_new_architecture",
    "update_selectors_from_results",
]
